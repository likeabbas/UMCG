diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 18b5500ea..519ab83d9 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -370,6 +370,7 @@
 446	common	landlock_restrict_self	sys_landlock_restrict_self
 447	common	memfd_secret		sys_memfd_secret
 448	common	process_mrelease	sys_process_mrelease
+449	common	gsys_umcg		sys_gsys_umcg
 
 #
 # Due to a historical design error, certain syscalls are numbered differently
diff --git a/fs/exec.c b/fs/exec.c
index a098c133d..dfa24bb99 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -1840,6 +1840,7 @@ static int bprm_execve(struct linux_binprm *bprm,
 	current->fs->in_exec = 0;
 	current->in_execve = 0;
 	rseq_execve(current);
+	umcg_execve(current);
 	acct_update_integrals(current);
 	task_numa_free(current, false);
 	return retval;
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 7f8ee09c7..de01510a6 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -560,6 +560,11 @@ struct mm_struct {
 
 		/* numa_scan_seq prevents two threads setting pte_numa */
 		int numa_scan_seq;
+#endif
+#ifdef CONFIG_UMCG
+		spinlock_t umcg_lock;
+		struct list_head umcg_idle_server_list;
+		struct task_struct *umcg_idle_workers;
 #endif
 		/*
 		 * An operation with batched TLB flushing is going on. Anything
diff --git a/include/linux/sched.h b/include/linux/sched.h
index c1a927dde..971a7fd18 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -11,6 +11,7 @@
 
 #include <asm/current.h>
 
+#include <linux/atomic.h>
 #include <linux/pid.h>
 #include <linux/sem.h>
 #include <linux/shm.h>
@@ -1294,6 +1295,25 @@ struct task_struct {
 	unsigned long rseq_event_mask;
 #endif
 
+#ifdef CONFIG_UMCG
+	/* UMCG server/worker status; see kernel/sched/umcg.c. */
+	atomic_long_t			umcg_status;
+	/* Set if this is a worker. */
+	struct task_struct		*umcg_server;
+	union {
+		/* If this is a worker. */
+		struct task_struct	*umcg_next_idle_worker;
+		/* If this is a server. */
+		struct list_head	umcg_idle_server_list;
+	};
+	union {
+		u64		umcg_worker_id;     /* If this is a worker. */
+		atomic_long_t	umcg_worker_event;  /* If this is a server. */
+	};
+	/* Worker jiffies are used to decide when to preempt a running worker. */
+	u64			umcg_worker_jiffies;
+#endif
+
 	struct tlbflush_unmap_batch	tlb_ubc;
 
 	union {
@@ -2285,6 +2305,47 @@ static inline void rseq_syscall(struct pt_regs *regs)
 
 #endif
 
+#ifdef CONFIG_UMCG
+
+/* Whether sched_submit_work() should call umcg_wq_worker_sleeping(). */
+extern bool umcg_wq_work(struct task_struct *tsk);
+
+extern void umcg_notify_resume(void);
+extern void umcg_tick(struct task_struct *curr);
+
+/* Called by do_exit() in kernel/exit.c. */
+extern void umcg_handle_exit(void);
+
+/* Called by bprm_execve() in fs/exec.c. */
+extern void umcg_execve(struct task_struct *tsk);
+
+/* umcg_wq_worker_sleeping() is called in core.c by sched_submit_work(). */
+extern void umcg_wq_worker_sleeping(struct task_struct *tsk);
+
+#else  /* CONFIG_UMCG */
+
+static inline bool umcg_wq_work(struct task_struct *tsk)
+{
+	return false;
+}
+static inline void umcg_notify_resume(void)
+{
+}
+static inline void umcg_tick(struct task_struct *curr)
+{
+}
+static inline void umcg_execve(struct task_struct *tsk)
+{
+}
+static inline void umcg_handle_exit(void)
+{
+}
+static inline void umcg_wq_worker_sleeping(struct task_struct *tsk)
+{
+}
+
+#endif
+
 const struct sched_avg *sched_trace_cfs_rq_avg(struct cfs_rq *cfs_rq);
 char *sched_trace_cfs_rq_path(struct cfs_rq *cfs_rq, char *str, int len);
 int sched_trace_cfs_rq_cpu(struct cfs_rq *cfs_rq);
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 252243c77..85d50de8f 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -1052,6 +1052,9 @@ asmlinkage long sys_landlock_add_rule(int ruleset_fd, enum landlock_rule_type ru
 		const void __user *rule_attr, __u32 flags);
 asmlinkage long sys_landlock_restrict_self(int ruleset_fd, __u32 flags);
 asmlinkage long sys_memfd_secret(unsigned int flags);
+asmlinkage long sys_gsys_umcg(u64 flags, u64 cmd, pid_t next_tid,
+		u64 abs_timeout, u64 __user *events,
+		int event_sz);
 
 /*
  * Architecture-specific system calls
diff --git a/include/linux/tracehook.h b/include/linux/tracehook.h
index 2564b7434..f1d5408fa 100644
--- a/include/linux/tracehook.h
+++ b/include/linux/tracehook.h
@@ -179,6 +179,8 @@ static inline void set_notify_resume(struct task_struct *task)
  */
 static inline void tracehook_notify_resume(struct pt_regs *regs)
 {
+	umcg_notify_resume();
+
 	clear_thread_flag(TIF_NOTIFY_RESUME);
 	/*
 	 * This barrier pairs with task_work_add()->set_notify_resume() after
diff --git a/include/uapi/asm-generic/unistd.h b/include/uapi/asm-generic/unistd.h
index 1c5fb86d4..0e17b28c5 100644
--- a/include/uapi/asm-generic/unistd.h
+++ b/include/uapi/asm-generic/unistd.h
@@ -880,8 +880,11 @@ __SYSCALL(__NR_memfd_secret, sys_memfd_secret)
 #define __NR_process_mrelease 448
 __SYSCALL(__NR_process_mrelease, sys_process_mrelease)
 
+#define __NR_gsys_umcg 449
+__SYSCALL(__NR_gsys_umcg, sys_gsys_umcg)
+
 #undef __NR_syscalls
-#define __NR_syscalls 449
+#define __NR_syscalls 450
 
 /*
  * 32 bit systems traditionally used different
diff --git a/include/uapi/linux/umcg.h b/include/uapi/linux/umcg.h
new file mode 100644
index 000000000..9e0c359ad
--- /dev/null
+++ b/include/uapi/linux/umcg.h
@@ -0,0 +1,54 @@
+/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
+#ifndef _UAPI_LINUX_UMCG_H
+#define _UAPI_LINUX_UMCG_H
+
+#include <linux/types.h>
+
+/*
+ * UMCG: User Managed Concurrency Groups.
+ */
+
+/**
+ * enum umcg_event_type - types of worker events delivered to UMCG servers.
+ *
+ * Note: worker events are delivered as the lower three bits of umcg_task_id.
+ */
+enum umcg_event_type {
+	UMCG_WE_BLOCK	= 1,
+	UMCG_WE_WAKE	= 2,
+	UMCG_WE_WAIT	= 3,
+	UMCG_WE_EXIT	= 4,
+	UMCG_WE_TIMEOUT	= 5,
+	UMCG_WE_PREEMPT	= 6,
+};
+
+/**
+ * enum umcg_cmd - sys_umcg commands.
+ */
+enum umcg_cmd {
+	UMCG_REGISTER_WORKER	= 1,
+	UMCG_REGISTER_SERVER	= 2,
+	UMCG_UNREGISTER		= 3,
+	UMCG_WAKE		= 4,  /* Wakes one idle server. */
+	UMCG_WAIT		= 5,
+	UMCG_CTX_SWITCH		= 6,
+};
+
+/*
+ * Pass this flag with UMCG_WAIT command if this is a repeated wait
+ * due to a signal.
+ */
+#define UMCG_WAIT_FLAG_INTERRUPTED (1ULL)
+
+/*
+ * UMCG worker IDs must have the last 5 bits as zeroes to OR with
+ * worker events.
+ */
+#define UMCG_WORKER_ID_ALIGNMENT (32ULL)
+
+/*
+ * int sys_umcg(u64 flags, u64 cmd, pid_t next_tid, u64 abs_timeout,
+ *              u64 __user *events, int event_sz);
+ */
+
+#endif /* _UAPI_LINUX_UMCG_H */
diff --git a/init/Kconfig b/init/Kconfig
index 11f8a845f..205fe084f 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1688,6 +1688,27 @@ config MEMBARRIER
 
 	  If unsure, say Y.
 
+config UMCG
+	bool "Enable User Managed Concurrency Groups API"
+	depends on 64BIT
+	default n
+	help
+	  Enable User Managed Concurrency Groups API, which form the basis
+	  for an in-process M:N userspace scheduling framework.
+	  At the moment this is an experimental/RFC feature that is not
+	  guaranteed to be backward-compatible.
+
+config UMCG_PREEMPT_JIFFIES
+	int "Interval at which running UMCG workers are preempted."
+	depends on UMCG
+	default 10
+	help
+	  Normally UMCG workers cooperatively "yield" their servers.
+	  However, UMCG worker preemption is needed in cases when
+	  the worker is waiting on a spinlock that a descheduled worker
+	  holds. In this case preempting a running worker gives the
+	  userspace scheduler a chance to run the lock-holding worker.
+
 config KALLSYMS
 	bool "Load all symbols for debugging/ksymoops" if EXPERT
 	default y
diff --git a/init/Kconfig.bak b/init/Kconfig.bak
new file mode 100644
index 000000000..401725a38
--- /dev/null
+++ b/init/Kconfig.bak
@@ -0,0 +1,2400 @@
+# SPDX-License-Identifier: GPL-2.0-only
+config CC_VERSION_TEXT
+	string
+	default "$(CC_VERSION_TEXT)"
+	help
+	  This is used in unclear ways:
+
+	  - Re-run Kconfig when the compiler is updated
+	    The 'default' property references the environment variable,
+	    CC_VERSION_TEXT so it is recorded in include/config/auto.conf.cmd.
+	    When the compiler is updated, Kconfig will be invoked.
+
+	  - Ensure full rebuild when the compiler is updated
+	    include/linux/compiler-version.h contains this option in the comment
+	    line so fixdep adds include/config/CC_VERSION_TEXT into the
+	    auto-generated dependency. When the compiler is updated, syncconfig
+	    will touch it and then every file will be rebuilt.
+
+config CC_IS_GCC
+	def_bool $(success,test "$(cc-name)" = GCC)
+
+config GCC_VERSION
+	int
+	default $(cc-version) if CC_IS_GCC
+	default 0
+
+config CC_IS_CLANG
+	def_bool $(success,test "$(cc-name)" = Clang)
+
+config CLANG_VERSION
+	int
+	default $(cc-version) if CC_IS_CLANG
+	default 0
+
+config AS_IS_GNU
+	def_bool $(success,test "$(as-name)" = GNU)
+
+config AS_IS_LLVM
+	def_bool $(success,test "$(as-name)" = LLVM)
+
+config AS_VERSION
+	int
+	# Use clang version if this is the integrated assembler
+	default CLANG_VERSION if AS_IS_LLVM
+	default $(as-version)
+
+config LD_IS_BFD
+	def_bool $(success,test "$(ld-name)" = BFD)
+
+config LD_VERSION
+	int
+	default $(ld-version) if LD_IS_BFD
+	default 0
+
+config LD_IS_LLD
+	def_bool $(success,test "$(ld-name)" = LLD)
+
+config LLD_VERSION
+	int
+	default $(ld-version) if LD_IS_LLD
+	default 0
+
+config CC_CAN_LINK
+	bool
+	default $(success,$(srctree)/scripts/cc-can-link.sh $(CC) $(CLANG_FLAGS) $(m64-flag)) if 64BIT
+	default $(success,$(srctree)/scripts/cc-can-link.sh $(CC) $(CLANG_FLAGS) $(m32-flag))
+
+config CC_CAN_LINK_STATIC
+	bool
+	default $(success,$(srctree)/scripts/cc-can-link.sh $(CC) $(CLANG_FLAGS) $(m64-flag) -static) if 64BIT
+	default $(success,$(srctree)/scripts/cc-can-link.sh $(CC) $(CLANG_FLAGS) $(m32-flag) -static)
+
+config CC_HAS_ASM_GOTO
+	def_bool $(success,$(srctree)/scripts/gcc-goto.sh $(CC))
+
+config CC_HAS_ASM_GOTO_OUTPUT
+	depends on CC_HAS_ASM_GOTO
+	def_bool $(success,echo 'int foo(int x) { asm goto ("": "=r"(x) ::: bar); return x; bar: return 0; }' | $(CC) -x c - -c -o /dev/null)
+
+config TOOLS_SUPPORT_RELR
+	def_bool $(success,env "CC=$(CC)" "LD=$(LD)" "NM=$(NM)" "OBJCOPY=$(OBJCOPY)" $(srctree)/scripts/tools-support-relr.sh)
+
+config CC_HAS_ASM_INLINE
+	def_bool $(success,echo 'void foo(void) { asm inline (""); }' | $(CC) -x c - -c -o /dev/null)
+
+config CC_HAS_NO_PROFILE_FN_ATTR
+	def_bool $(success,echo '__attribute__((no_profile_instrument_function)) int x();' | $(CC) -x c - -c -o /dev/null -Werror)
+
+config CONSTRUCTORS
+	bool
+
+config IRQ_WORK
+	bool
+
+config BUILDTIME_TABLE_SORT
+	bool
+
+config THREAD_INFO_IN_TASK
+	bool
+	help
+	  Select this to move thread_info off the stack into task_struct.  To
+	  make this work, an arch will need to remove all thread_info fields
+	  except flags and fix any runtime bugs.
+
+	  One subtle change that will be needed is to use try_get_task_stack()
+	  and put_task_stack() in save_thread_stack_tsk() and get_wchan().
+
+menu "General setup"
+
+config BROKEN
+	bool
+
+config BROKEN_ON_SMP
+	bool
+	depends on BROKEN || !SMP
+	default y
+
+config INIT_ENV_ARG_LIMIT
+	int
+	default 32 if !UML
+	default 128 if UML
+	help
+	  Maximum of each of the number of arguments and environment
+	  variables passed to init from the kernel command line.
+
+config COMPILE_TEST
+	bool "Compile also drivers which will not load"
+	depends on HAS_IOMEM
+	help
+	  Some drivers can be compiled on a different platform than they are
+	  intended to be run on. Despite they cannot be loaded there (or even
+	  when they load they cannot be used due to missing HW support),
+	  developers still, opposing to distributors, might want to build such
+	  drivers to compile-test them.
+
+	  If you are a developer and want to build everything available, say Y
+	  here. If you are a user/distributor, say N here to exclude useless
+	  drivers to be distributed.
+
+config WERROR
+	bool "Compile the kernel with warnings as errors"
+	default COMPILE_TEST
+	help
+	  A kernel build should not cause any compiler warnings, and this
+	  enables the '-Werror' flag to enforce that rule by default.
+
+	  However, if you have a new (or very old) compiler with odd and
+	  unusual warnings, or you have some architecture with problems,
+	  you may need to disable this config option in order to
+	  successfully build the kernel.
+
+	  If in doubt, say Y.
+
+config UAPI_HEADER_TEST
+	bool "Compile test UAPI headers"
+	depends on HEADERS_INSTALL && CC_CAN_LINK
+	help
+	  Compile test headers exported to user-space to ensure they are
+	  self-contained, i.e. compilable as standalone units.
+
+	  If you are a developer or tester and want to ensure the exported
+	  headers are self-contained, say Y here. Otherwise, choose N.
+
+config LOCALVERSION
+	string "Local version - append to kernel release"
+	help
+	  Append an extra string to the end of your kernel version.
+	  This will show up when you type uname, for example.
+	  The string you set here will be appended after the contents of
+	  any files with a filename matching localversion* in your
+	  object and source tree, in that order.  Your total string can
+	  be a maximum of 64 characters.
+
+config LOCALVERSION_AUTO
+	bool "Automatically append version information to the version string"
+	default y
+	depends on !COMPILE_TEST
+	help
+	  This will try to automatically determine if the current tree is a
+	  release tree by looking for git tags that belong to the current
+	  top of tree revision.
+
+	  A string of the format -gxxxxxxxx will be added to the localversion
+	  if a git-based tree is found.  The string generated by this will be
+	  appended after any matching localversion* files, and after the value
+	  set in CONFIG_LOCALVERSION.
+
+	  (The actual string used here is the first eight characters produced
+	  by running the command:
+
+	    $ git rev-parse --verify HEAD
+
+	  which is done within the script "scripts/setlocalversion".)
+
+config BUILD_SALT
+	string "Build ID Salt"
+	default ""
+	help
+	  The build ID is used to link binaries and their debug info. Setting
+	  this option will use the value in the calculation of the build id.
+	  This is mostly useful for distributions which want to ensure the
+	  build is unique between builds. It's safe to leave the default.
+
+config HAVE_KERNEL_GZIP
+	bool
+
+config HAVE_KERNEL_BZIP2
+	bool
+
+config HAVE_KERNEL_LZMA
+	bool
+
+config HAVE_KERNEL_XZ
+	bool
+
+config HAVE_KERNEL_LZO
+	bool
+
+config HAVE_KERNEL_LZ4
+	bool
+
+config HAVE_KERNEL_ZSTD
+	bool
+
+config HAVE_KERNEL_UNCOMPRESSED
+	bool
+
+choice
+	prompt "Kernel compression mode"
+	default KERNEL_GZIP
+	depends on HAVE_KERNEL_GZIP || HAVE_KERNEL_BZIP2 || HAVE_KERNEL_LZMA || HAVE_KERNEL_XZ || HAVE_KERNEL_LZO || HAVE_KERNEL_LZ4 || HAVE_KERNEL_ZSTD || HAVE_KERNEL_UNCOMPRESSED
+	help
+	  The linux kernel is a kind of self-extracting executable.
+	  Several compression algorithms are available, which differ
+	  in efficiency, compression and decompression speed.
+	  Compression speed is only relevant when building a kernel.
+	  Decompression speed is relevant at each boot.
+
+	  If you have any problems with bzip2 or lzma compressed
+	  kernels, mail me (Alain Knaff) <alain@knaff.lu>. (An older
+	  version of this functionality (bzip2 only), for 2.4, was
+	  supplied by Christian Ludwig)
+
+	  High compression options are mostly useful for users, who
+	  are low on disk space (embedded systems), but for whom ram
+	  size matters less.
+
+	  If in doubt, select 'gzip'
+
+config KERNEL_GZIP
+	bool "Gzip"
+	depends on HAVE_KERNEL_GZIP
+	help
+	  The old and tried gzip compression. It provides a good balance
+	  between compression ratio and decompression speed.
+
+config KERNEL_BZIP2
+	bool "Bzip2"
+	depends on HAVE_KERNEL_BZIP2
+	help
+	  Its compression ratio and speed is intermediate.
+	  Decompression speed is slowest among the choices.  The kernel
+	  size is about 10% smaller with bzip2, in comparison to gzip.
+	  Bzip2 uses a large amount of memory. For modern kernels you
+	  will need at least 8MB RAM or more for booting.
+
+config KERNEL_LZMA
+	bool "LZMA"
+	depends on HAVE_KERNEL_LZMA
+	help
+	  This compression algorithm's ratio is best.  Decompression speed
+	  is between gzip and bzip2.  Compression is slowest.
+	  The kernel size is about 33% smaller with LZMA in comparison to gzip.
+
+config KERNEL_XZ
+	bool "XZ"
+	depends on HAVE_KERNEL_XZ
+	help
+	  XZ uses the LZMA2 algorithm and instruction set specific
+	  BCJ filters which can improve compression ratio of executable
+	  code. The size of the kernel is about 30% smaller with XZ in
+	  comparison to gzip. On architectures for which there is a BCJ
+	  filter (i386, x86_64, ARM, IA-64, PowerPC, and SPARC), XZ
+	  will create a few percent smaller kernel than plain LZMA.
+
+	  The speed is about the same as with LZMA: The decompression
+	  speed of XZ is better than that of bzip2 but worse than gzip
+	  and LZO. Compression is slow.
+
+config KERNEL_LZO
+	bool "LZO"
+	depends on HAVE_KERNEL_LZO
+	help
+	  Its compression ratio is the poorest among the choices. The kernel
+	  size is about 10% bigger than gzip; however its speed
+	  (both compression and decompression) is the fastest.
+
+config KERNEL_LZ4
+	bool "LZ4"
+	depends on HAVE_KERNEL_LZ4
+	help
+	  LZ4 is an LZ77-type compressor with a fixed, byte-oriented encoding.
+	  A preliminary version of LZ4 de/compression tool is available at
+	  <https://code.google.com/p/lz4/>.
+
+	  Its compression ratio is worse than LZO. The size of the kernel
+	  is about 8% bigger than LZO. But the decompression speed is
+	  faster than LZO.
+
+config KERNEL_ZSTD
+	bool "ZSTD"
+	depends on HAVE_KERNEL_ZSTD
+	help
+	  ZSTD is a compression algorithm targeting intermediate compression
+	  with fast decompression speed. It will compress better than GZIP and
+	  decompress around the same speed as LZO, but slower than LZ4. You
+	  will need at least 192 KB RAM or more for booting. The zstd command
+	  line tool is required for compression.
+
+config KERNEL_UNCOMPRESSED
+	bool "None"
+	depends on HAVE_KERNEL_UNCOMPRESSED
+	help
+	  Produce uncompressed kernel image. This option is usually not what
+	  you want. It is useful for debugging the kernel in slow simulation
+	  environments, where decompressing and moving the kernel is awfully
+	  slow. This option allows early boot code to skip the decompressor
+	  and jump right at uncompressed kernel image.
+
+endchoice
+
+config DEFAULT_INIT
+	string "Default init path"
+	default ""
+	help
+	  This option determines the default init for the system if no init=
+	  option is passed on the kernel command line. If the requested path is
+	  not present, we will still then move on to attempting further
+	  locations (e.g. /sbin/init, etc). If this is empty, we will just use
+	  the fallback list when init= is not passed.
+
+config DEFAULT_HOSTNAME
+	string "Default hostname"
+	default "(none)"
+	help
+	  This option determines the default system hostname before userspace
+	  calls sethostname(2). The kernel traditionally uses "(none)" here,
+	  but you may wish to use a different default here to make a minimal
+	  system more usable with less configuration.
+
+#
+# For some reason microblaze and nios2 hard code SWAP=n.  Hopefully we can
+# add proper SWAP support to them, in which case this can be remove.
+#
+config ARCH_NO_SWAP
+	bool
+
+config SWAP
+	bool "Support for paging of anonymous memory (swap)"
+	depends on MMU && BLOCK && !ARCH_NO_SWAP
+	default y
+	help
+	  This option allows you to choose whether you want to have support
+	  for so called swap devices or swap files in your kernel that are
+	  used to provide more virtual memory than the actual RAM present
+	  in your computer.  If unsure say Y.
+
+config SYSVIPC
+	bool "System V IPC"
+	help
+	  Inter Process Communication is a suite of library functions and
+	  system calls which let processes (running programs) synchronize and
+	  exchange information. It is generally considered to be a good thing,
+	  and some programs won't run unless you say Y here. In particular, if
+	  you want to run the DOS emulator dosemu under Linux (read the
+	  DOSEMU-HOWTO, available from <http://www.tldp.org/docs.html#howto>),
+	  you'll need to say Y here.
+
+	  You can find documentation about IPC with "info ipc" and also in
+	  section 6.4 of the Linux Programmer's Guide, available from
+	  <http://www.tldp.org/guides.html>.
+
+config SYSVIPC_SYSCTL
+	bool
+	depends on SYSVIPC
+	depends on SYSCTL
+	default y
+
+config POSIX_MQUEUE
+	bool "POSIX Message Queues"
+	depends on NET
+	help
+	  POSIX variant of message queues is a part of IPC. In POSIX message
+	  queues every message has a priority which decides about succession
+	  of receiving it by a process. If you want to compile and run
+	  programs written e.g. for Solaris with use of its POSIX message
+	  queues (functions mq_*) say Y here.
+
+	  POSIX message queues are visible as a filesystem called 'mqueue'
+	  and can be mounted somewhere if you want to do filesystem
+	  operations on message queues.
+
+	  If unsure, say Y.
+
+config POSIX_MQUEUE_SYSCTL
+	bool
+	depends on POSIX_MQUEUE
+	depends on SYSCTL
+	default y
+
+config WATCH_QUEUE
+	bool "General notification queue"
+	default n
+	help
+
+	  This is a general notification queue for the kernel to pass events to
+	  userspace by splicing them into pipes.  It can be used in conjunction
+	  with watches for key/keyring change notifications and device
+	  notifications.
+
+	  See Documentation/watch_queue.rst
+
+config CROSS_MEMORY_ATTACH
+	bool "Enable process_vm_readv/writev syscalls"
+	depends on MMU
+	default y
+	help
+	  Enabling this option adds the system calls process_vm_readv and
+	  process_vm_writev which allow a process with the correct privileges
+	  to directly read from or write to another process' address space.
+	  See the man page for more details.
+
+config USELIB
+	bool "uselib syscall"
+	def_bool ALPHA || M68K || SPARC || X86_32 || IA32_EMULATION
+	help
+	  This option enables the uselib syscall, a system call used in the
+	  dynamic linker from libc5 and earlier.  glibc does not use this
+	  system call.  If you intend to run programs built on libc5 or
+	  earlier, you may need to enable this syscall.  Current systems
+	  running glibc can safely disable this.
+
+config AUDIT
+	bool "Auditing support"
+	depends on NET
+	help
+	  Enable auditing infrastructure that can be used with another
+	  kernel subsystem, such as SELinux (which requires this for
+	  logging of avc messages output).  System call auditing is included
+	  on architectures which support it.
+
+config HAVE_ARCH_AUDITSYSCALL
+	bool
+
+config AUDITSYSCALL
+	def_bool y
+	depends on AUDIT && HAVE_ARCH_AUDITSYSCALL
+	select FSNOTIFY
+
+source "kernel/irq/Kconfig"
+source "kernel/time/Kconfig"
+source "kernel/bpf/Kconfig"
+source "kernel/Kconfig.preempt"
+
+menu "CPU/Task time and stats accounting"
+
+config VIRT_CPU_ACCOUNTING
+	bool
+
+choice
+	prompt "Cputime accounting"
+	default TICK_CPU_ACCOUNTING if !PPC64
+	default VIRT_CPU_ACCOUNTING_NATIVE if PPC64
+
+# Kind of a stub config for the pure tick based cputime accounting
+config TICK_CPU_ACCOUNTING
+	bool "Simple tick based cputime accounting"
+	depends on !S390 && !NO_HZ_FULL
+	help
+	  This is the basic tick based cputime accounting that maintains
+	  statistics about user, system and idle time spent on per jiffies
+	  granularity.
+
+	  If unsure, say Y.
+
+config VIRT_CPU_ACCOUNTING_NATIVE
+	bool "Deterministic task and CPU time accounting"
+	depends on HAVE_VIRT_CPU_ACCOUNTING && !NO_HZ_FULL
+	select VIRT_CPU_ACCOUNTING
+	help
+	  Select this option to enable more accurate task and CPU time
+	  accounting.  This is done by reading a CPU counter on each
+	  kernel entry and exit and on transitions within the kernel
+	  between system, softirq and hardirq state, so there is a
+	  small performance impact.  In the case of s390 or IBM POWER > 5,
+	  this also enables accounting of stolen time on logically-partitioned
+	  systems.
+
+config VIRT_CPU_ACCOUNTING_GEN
+	bool "Full dynticks CPU time accounting"
+	depends on HAVE_CONTEXT_TRACKING
+	depends on HAVE_VIRT_CPU_ACCOUNTING_GEN
+	depends on GENERIC_CLOCKEVENTS
+	select VIRT_CPU_ACCOUNTING
+	select CONTEXT_TRACKING
+	help
+	  Select this option to enable task and CPU time accounting on full
+	  dynticks systems. This accounting is implemented by watching every
+	  kernel-user boundaries using the context tracking subsystem.
+	  The accounting is thus performed at the expense of some significant
+	  overhead.
+
+	  For now this is only useful if you are working on the full
+	  dynticks subsystem development.
+
+	  If unsure, say N.
+
+endchoice
+
+config IRQ_TIME_ACCOUNTING
+	bool "Fine granularity task level IRQ time accounting"
+	depends on HAVE_IRQ_TIME_ACCOUNTING && !VIRT_CPU_ACCOUNTING_NATIVE
+	help
+	  Select this option to enable fine granularity task irq time
+	  accounting. This is done by reading a timestamp on each
+	  transitions between softirq and hardirq state, so there can be a
+	  small performance impact.
+
+	  If in doubt, say N here.
+
+config HAVE_SCHED_AVG_IRQ
+	def_bool y
+	depends on IRQ_TIME_ACCOUNTING || PARAVIRT_TIME_ACCOUNTING
+	depends on SMP
+
+config SCHED_THERMAL_PRESSURE
+	bool
+	default y if ARM && ARM_CPU_TOPOLOGY
+	default y if ARM64
+	depends on SMP
+	depends on CPU_FREQ_THERMAL
+	help
+	  Select this option to enable thermal pressure accounting in the
+	  scheduler. Thermal pressure is the value conveyed to the scheduler
+	  that reflects the reduction in CPU compute capacity resulted from
+	  thermal throttling. Thermal throttling occurs when the performance of
+	  a CPU is capped due to high operating temperatures.
+
+	  If selected, the scheduler will be able to balance tasks accordingly,
+	  i.e. put less load on throttled CPUs than on non/less throttled ones.
+
+	  This requires the architecture to implement
+	  arch_set_thermal_pressure() and arch_scale_thermal_pressure().
+
+config BSD_PROCESS_ACCT
+	bool "BSD Process Accounting"
+	depends on MULTIUSER
+	help
+	  If you say Y here, a user level program will be able to instruct the
+	  kernel (via a special system call) to write process accounting
+	  information to a file: whenever a process exits, information about
+	  that process will be appended to the file by the kernel.  The
+	  information includes things such as creation time, owning user,
+	  command name, memory usage, controlling terminal etc. (the complete
+	  list is in the struct acct in <file:include/linux/acct.h>).  It is
+	  up to the user level program to do useful things with this
+	  information.  This is generally a good idea, so say Y.
+
+config BSD_PROCESS_ACCT_V3
+	bool "BSD Process Accounting version 3 file format"
+	depends on BSD_PROCESS_ACCT
+	default n
+	help
+	  If you say Y here, the process accounting information is written
+	  in a new file format that also logs the process IDs of each
+	  process and its parent. Note that this file format is incompatible
+	  with previous v0/v1/v2 file formats, so you will need updated tools
+	  for processing it. A preliminary version of these tools is available
+	  at <http://www.gnu.org/software/acct/>.
+
+config TASKSTATS
+	bool "Export task/process statistics through netlink"
+	depends on NET
+	depends on MULTIUSER
+	default n
+	help
+	  Export selected statistics for tasks/processes through the
+	  generic netlink interface. Unlike BSD process accounting, the
+	  statistics are available during the lifetime of tasks/processes as
+	  responses to commands. Like BSD accounting, they are sent to user
+	  space on task exit.
+
+	  Say N if unsure.
+
+config TASK_DELAY_ACCT
+	bool "Enable per-task delay accounting"
+	depends on TASKSTATS
+	select SCHED_INFO
+	help
+	  Collect information on time spent by a task waiting for system
+	  resources like cpu, synchronous block I/O completion and swapping
+	  in pages. Such statistics can help in setting a task's priorities
+	  relative to other tasks for cpu, io, rss limits etc.
+
+	  Say N if unsure.
+
+config TASK_XACCT
+	bool "Enable extended accounting over taskstats"
+	depends on TASKSTATS
+	help
+	  Collect extended task accounting data and send the data
+	  to userland for processing over the taskstats interface.
+
+	  Say N if unsure.
+
+config TASK_IO_ACCOUNTING
+	bool "Enable per-task storage I/O accounting"
+	depends on TASK_XACCT
+	help
+	  Collect information on the number of bytes of storage I/O which this
+	  task has caused.
+
+	  Say N if unsure.
+
+config PSI
+	bool "Pressure stall information tracking"
+	help
+	  Collect metrics that indicate how overcommitted the CPU, memory,
+	  and IO capacity are in the system.
+
+	  If you say Y here, the kernel will create /proc/pressure/ with the
+	  pressure statistics files cpu, memory, and io. These will indicate
+	  the share of walltime in which some or all tasks in the system are
+	  delayed due to contention of the respective resource.
+
+	  In kernels with cgroup support, cgroups (cgroup2 only) will
+	  have cpu.pressure, memory.pressure, and io.pressure files,
+	  which aggregate pressure stalls for the grouped tasks only.
+
+	  For more details see Documentation/accounting/psi.rst.
+
+	  Say N if unsure.
+
+config PSI_DEFAULT_DISABLED
+	bool "Require boot parameter to enable pressure stall information tracking"
+	default n
+	depends on PSI
+	help
+	  If set, pressure stall information tracking will be disabled
+	  per default but can be enabled through passing psi=1 on the
+	  kernel commandline during boot.
+
+	  This feature adds some code to the task wakeup and sleep
+	  paths of the scheduler. The overhead is too low to affect
+	  common scheduling-intense workloads in practice (such as
+	  webservers, memcache), but it does show up in artificial
+	  scheduler stress tests, such as hackbench.
+
+	  If you are paranoid and not sure what the kernel will be
+	  used for, say Y.
+
+	  Say N if unsure.
+
+endmenu # "CPU/Task time and stats accounting"
+
+config CPU_ISOLATION
+	bool "CPU isolation"
+	depends on SMP || COMPILE_TEST
+	default y
+	help
+	  Make sure that CPUs running critical tasks are not disturbed by
+	  any source of "noise" such as unbound workqueues, timers, kthreads...
+	  Unbound jobs get offloaded to housekeeping CPUs. This is driven by
+	  the "isolcpus=" boot parameter.
+
+	  Say Y if unsure.
+
+source "kernel/rcu/Kconfig"
+
+config BUILD_BIN2C
+	bool
+	default n
+
+config IKCONFIG
+	tristate "Kernel .config support"
+	help
+	  This option enables the complete Linux kernel ".config" file
+	  contents to be saved in the kernel. It provides documentation
+	  of which kernel options are used in a running kernel or in an
+	  on-disk kernel.  This information can be extracted from the kernel
+	  image file with the script scripts/extract-ikconfig and used as
+	  input to rebuild the current kernel or to build another kernel.
+	  It can also be extracted from a running kernel by reading
+	  /proc/config.gz if enabled (below).
+
+config IKCONFIG_PROC
+	bool "Enable access to .config through /proc/config.gz"
+	depends on IKCONFIG && PROC_FS
+	help
+	  This option enables access to the kernel configuration file
+	  through /proc/config.gz.
+
+config IKHEADERS
+	tristate "Enable kernel headers through /sys/kernel/kheaders.tar.xz"
+	depends on SYSFS
+	help
+	  This option enables access to the in-kernel headers that are generated during
+	  the build process. These can be used to build eBPF tracing programs,
+	  or similar programs.  If you build the headers as a module, a module called
+	  kheaders.ko is built which can be loaded on-demand to get access to headers.
+
+config LOG_BUF_SHIFT
+	int "Kernel log buffer size (16 => 64KB, 17 => 128KB)"
+	range 12 25 if !H8300
+	range 12 19 if H8300
+	default 17
+	depends on PRINTK
+	help
+	  Select the minimal kernel log buffer size as a power of 2.
+	  The final size is affected by LOG_CPU_MAX_BUF_SHIFT config
+	  parameter, see below. Any higher size also might be forced
+	  by "log_buf_len" boot parameter.
+
+	  Examples:
+		     17 => 128 KB
+		     16 => 64 KB
+		     15 => 32 KB
+		     14 => 16 KB
+		     13 =>  8 KB
+		     12 =>  4 KB
+
+config LOG_CPU_MAX_BUF_SHIFT
+	int "CPU kernel log buffer size contribution (13 => 8 KB, 17 => 128KB)"
+	depends on SMP
+	range 0 21
+	default 12 if !BASE_SMALL
+	default 0 if BASE_SMALL
+	depends on PRINTK
+	help
+	  This option allows to increase the default ring buffer size
+	  according to the number of CPUs. The value defines the contribution
+	  of each CPU as a power of 2. The used space is typically only few
+	  lines however it might be much more when problems are reported,
+	  e.g. backtraces.
+
+	  The increased size means that a new buffer has to be allocated and
+	  the original static one is unused. It makes sense only on systems
+	  with more CPUs. Therefore this value is used only when the sum of
+	  contributions is greater than the half of the default kernel ring
+	  buffer as defined by LOG_BUF_SHIFT. The default values are set
+	  so that more than 16 CPUs are needed to trigger the allocation.
+
+	  Also this option is ignored when "log_buf_len" kernel parameter is
+	  used as it forces an exact (power of two) size of the ring buffer.
+
+	  The number of possible CPUs is used for this computation ignoring
+	  hotplugging making the computation optimal for the worst case
+	  scenario while allowing a simple algorithm to be used from bootup.
+
+	  Examples shift values and their meaning:
+		     17 => 128 KB for each CPU
+		     16 =>  64 KB for each CPU
+		     15 =>  32 KB for each CPU
+		     14 =>  16 KB for each CPU
+		     13 =>   8 KB for each CPU
+		     12 =>   4 KB for each CPU
+
+config PRINTK_SAFE_LOG_BUF_SHIFT
+	int "Temporary per-CPU printk log buffer size (12 => 4KB, 13 => 8KB)"
+	range 10 21
+	default 13
+	depends on PRINTK
+	help
+	  Select the size of an alternate printk per-CPU buffer where messages
+	  printed from usafe contexts are temporary stored. One example would
+	  be NMI messages, another one - printk recursion. The messages are
+	  copied to the main log buffer in a safe context to avoid a deadlock.
+	  The value defines the size as a power of 2.
+
+	  Those messages are rare and limited. The largest one is when
+	  a backtrace is printed. It usually fits into 4KB. Select
+	  8KB if you want to be on the safe side.
+
+	  Examples:
+		     17 => 128 KB for each CPU
+		     16 =>  64 KB for each CPU
+		     15 =>  32 KB for each CPU
+		     14 =>  16 KB for each CPU
+		     13 =>   8 KB for each CPU
+		     12 =>   4 KB for each CPU
+
+config PRINTK_INDEX
+	bool "Printk indexing debugfs interface"
+	depends on PRINTK && DEBUG_FS
+	help
+	  Add support for indexing of all printk formats known at compile time
+	  at <debugfs>/printk/index/<module>.
+
+	  This can be used as part of maintaining daemons which monitor
+	  /dev/kmsg, as it permits auditing the printk formats present in a
+	  kernel, allowing detection of cases where monitored printks are
+	  changed or no longer present.
+
+	  There is no additional runtime cost to printk with this enabled.
+
+#
+# Architectures with an unreliable sched_clock() should select this:
+#
+config HAVE_UNSTABLE_SCHED_CLOCK
+	bool
+
+config GENERIC_SCHED_CLOCK
+	bool
+
+menu "Scheduler features"
+
+config UCLAMP_TASK
+	bool "Enable utilization clamping for RT/FAIR tasks"
+	depends on CPU_FREQ_GOV_SCHEDUTIL
+	help
+	  This feature enables the scheduler to track the clamped utilization
+	  of each CPU based on RUNNABLE tasks scheduled on that CPU.
+
+	  With this option, the user can specify the min and max CPU
+	  utilization allowed for RUNNABLE tasks. The max utilization defines
+	  the maximum frequency a task should use while the min utilization
+	  defines the minimum frequency it should use.
+
+	  Both min and max utilization clamp values are hints to the scheduler,
+	  aiming at improving its frequency selection policy, but they do not
+	  enforce or grant any specific bandwidth for tasks.
+
+	  If in doubt, say N.
+
+config UCLAMP_BUCKETS_COUNT
+	int "Number of supported utilization clamp buckets"
+	range 5 20
+	default 5
+	depends on UCLAMP_TASK
+	help
+	  Defines the number of clamp buckets to use. The range of each bucket
+	  will be SCHED_CAPACITY_SCALE/UCLAMP_BUCKETS_COUNT. The higher the
+	  number of clamp buckets the finer their granularity and the higher
+	  the precision of clamping aggregation and tracking at run-time.
+
+	  For example, with the minimum configuration value we will have 5
+	  clamp buckets tracking 20% utilization each. A 25% boosted tasks will
+	  be refcounted in the [20..39]% bucket and will set the bucket clamp
+	  effective value to 25%.
+	  If a second 30% boosted task should be co-scheduled on the same CPU,
+	  that task will be refcounted in the same bucket of the first task and
+	  it will boost the bucket clamp effective value to 30%.
+	  The clamp effective value of a bucket is reset to its nominal value
+	  (20% in the example above) when there are no more tasks refcounted in
+	  that bucket.
+
+	  An additional boost/capping margin can be added to some tasks. In the
+	  example above the 25% task will be boosted to 30% until it exits the
+	  CPU. If that should be considered not acceptable on certain systems,
+	  it's always possible to reduce the margin by increasing the number of
+	  clamp buckets to trade off used memory for run-time tracking
+	  precision.
+
+	  If in doubt, use the default value.
+
+endmenu
+
+#
+# For architectures that want to enable the support for NUMA-affine scheduler
+# balancing logic:
+#
+config ARCH_SUPPORTS_NUMA_BALANCING
+	bool
+
+#
+# For architectures that prefer to flush all TLBs after a number of pages
+# are unmapped instead of sending one IPI per page to flush. The architecture
+# must provide guarantees on what happens if a clean TLB cache entry is
+# written after the unmap. Details are in mm/rmap.c near the check for
+# should_defer_flush. The architecture should also consider if the full flush
+# and the refill costs are offset by the savings of sending fewer IPIs.
+config ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
+	bool
+
+config CC_HAS_INT128
+	def_bool !$(cc-option,$(m64-flag) -D__SIZEOF_INT128__=0) && 64BIT
+
+#
+# For architectures that know their GCC __int128 support is sound
+#
+config ARCH_SUPPORTS_INT128
+	bool
+
+# For architectures that (ab)use NUMA to represent different memory regions
+# all cpu-local but of different latencies, such as SuperH.
+#
+config ARCH_WANT_NUMA_VARIABLE_LOCALITY
+	bool
+
+config NUMA_BALANCING
+	bool "Memory placement aware NUMA scheduler"
+	depends on ARCH_SUPPORTS_NUMA_BALANCING
+	depends on !ARCH_WANT_NUMA_VARIABLE_LOCALITY
+	depends on SMP && NUMA && MIGRATION
+	help
+	  This option adds support for automatic NUMA aware memory/task placement.
+	  The mechanism is quite primitive and is based on migrating memory when
+	  it has references to the node the task is running on.
+
+	  This system will be inactive on UMA systems.
+
+config NUMA_BALANCING_DEFAULT_ENABLED
+	bool "Automatically enable NUMA aware memory/task placement"
+	default y
+	depends on NUMA_BALANCING
+	help
+	  If set, automatic NUMA balancing will be enabled if running on a NUMA
+	  machine.
+
+menuconfig CGROUPS
+	bool "Control Group support"
+	select KERNFS
+	help
+	  This option adds support for grouping sets of processes together, for
+	  use with process control subsystems such as Cpusets, CFS, memory
+	  controls or device isolation.
+	  See
+		- Documentation/scheduler/sched-design-CFS.rst	(CFS)
+		- Documentation/admin-guide/cgroup-v1/ (features for grouping, isolation
+					  and resource control)
+
+	  Say N if unsure.
+
+if CGROUPS
+
+config PAGE_COUNTER
+	bool
+
+config MEMCG
+	bool "Memory controller"
+	select PAGE_COUNTER
+	select EVENTFD
+	help
+	  Provides control over the memory footprint of tasks in a cgroup.
+
+config MEMCG_SWAP
+	bool
+	depends on MEMCG && SWAP
+	default y
+
+config MEMCG_KMEM
+	bool
+	depends on MEMCG && !SLOB
+	default y
+
+config BLK_CGROUP
+	bool "IO controller"
+	depends on BLOCK
+	default n
+	help
+	Generic block IO controller cgroup interface. This is the common
+	cgroup interface which should be used by various IO controlling
+	policies.
+
+	Currently, CFQ IO scheduler uses it to recognize task groups and
+	control disk bandwidth allocation (proportional time slice allocation)
+	to such task groups. It is also used by bio throttling logic in
+	block layer to implement upper limit in IO rates on a device.
+
+	This option only enables generic Block IO controller infrastructure.
+	One needs to also enable actual IO controlling logic/policy. For
+	enabling proportional weight division of disk bandwidth in CFQ, set
+	CONFIG_BFQ_GROUP_IOSCHED=y; for enabling throttling policy, set
+	CONFIG_BLK_DEV_THROTTLING=y.
+
+	See Documentation/admin-guide/cgroup-v1/blkio-controller.rst for more information.
+
+config CGROUP_WRITEBACK
+	bool
+	depends on MEMCG && BLK_CGROUP
+	default y
+
+menuconfig CGROUP_SCHED
+	bool "CPU controller"
+	default n
+	help
+	  This feature lets CPU scheduler recognize task groups and control CPU
+	  bandwidth allocation to such task groups. It uses cgroups to group
+	  tasks.
+
+if CGROUP_SCHED
+config FAIR_GROUP_SCHED
+	bool "Group scheduling for SCHED_OTHER"
+	depends on CGROUP_SCHED
+	default CGROUP_SCHED
+
+config CFS_BANDWIDTH
+	bool "CPU bandwidth provisioning for FAIR_GROUP_SCHED"
+	depends on FAIR_GROUP_SCHED
+	default n
+	help
+	  This option allows users to define CPU bandwidth rates (limits) for
+	  tasks running within the fair group scheduler.  Groups with no limit
+	  set are considered to be unconstrained and will run with no
+	  restriction.
+	  See Documentation/scheduler/sched-bwc.rst for more information.
+
+config RT_GROUP_SCHED
+	bool "Group scheduling for SCHED_RR/FIFO"
+	depends on CGROUP_SCHED
+	default n
+	help
+	  This feature lets you explicitly allocate real CPU bandwidth
+	  to task groups. If enabled, it will also make it impossible to
+	  schedule realtime tasks for non-root users until you allocate
+	  realtime bandwidth for them.
+	  See Documentation/scheduler/sched-rt-group.rst for more information.
+
+endif #CGROUP_SCHED
+
+config UCLAMP_TASK_GROUP
+	bool "Utilization clamping per group of tasks"
+	depends on CGROUP_SCHED
+	depends on UCLAMP_TASK
+	default n
+	help
+	  This feature enables the scheduler to track the clamped utilization
+	  of each CPU based on RUNNABLE tasks currently scheduled on that CPU.
+
+	  When this option is enabled, the user can specify a min and max
+	  CPU bandwidth which is allowed for each single task in a group.
+	  The max bandwidth allows to clamp the maximum frequency a task
+	  can use, while the min bandwidth allows to define a minimum
+	  frequency a task will always use.
+
+	  When task group based utilization clamping is enabled, an eventually
+	  specified task-specific clamp value is constrained by the cgroup
+	  specified clamp value. Both minimum and maximum task clamping cannot
+	  be bigger than the corresponding clamping defined at task group level.
+
+	  If in doubt, say N.
+
+config CGROUP_PIDS
+	bool "PIDs controller"
+	help
+	  Provides enforcement of process number limits in the scope of a
+	  cgroup. Any attempt to fork more processes than is allowed in the
+	  cgroup will fail. PIDs are fundamentally a global resource because it
+	  is fairly trivial to reach PID exhaustion before you reach even a
+	  conservative kmemcg limit. As a result, it is possible to grind a
+	  system to halt without being limited by other cgroup policies. The
+	  PIDs controller is designed to stop this from happening.
+
+	  It should be noted that organisational operations (such as attaching
+	  to a cgroup hierarchy) will *not* be blocked by the PIDs controller,
+	  since the PIDs limit only affects a process's ability to fork, not to
+	  attach to a cgroup.
+
+config CGROUP_RDMA
+	bool "RDMA controller"
+	help
+	  Provides enforcement of RDMA resources defined by IB stack.
+	  It is fairly easy for consumers to exhaust RDMA resources, which
+	  can result into resource unavailability to other consumers.
+	  RDMA controller is designed to stop this from happening.
+	  Attaching processes with active RDMA resources to the cgroup
+	  hierarchy is allowed even if can cross the hierarchy's limit.
+
+config CGROUP_FREEZER
+	bool "Freezer controller"
+	help
+	  Provides a way to freeze and unfreeze all tasks in a
+	  cgroup.
+
+	  This option affects the ORIGINAL cgroup interface. The cgroup2 memory
+	  controller includes important in-kernel memory consumers per default.
+
+	  If you're using cgroup2, say N.
+
+config CGROUP_HUGETLB
+	bool "HugeTLB controller"
+	depends on HUGETLB_PAGE
+	select PAGE_COUNTER
+	default n
+	help
+	  Provides a cgroup controller for HugeTLB pages.
+	  When you enable this, you can put a per cgroup limit on HugeTLB usage.
+	  The limit is enforced during page fault. Since HugeTLB doesn't
+	  support page reclaim, enforcing the limit at page fault time implies
+	  that, the application will get SIGBUS signal if it tries to access
+	  HugeTLB pages beyond its limit. This requires the application to know
+	  beforehand how much HugeTLB pages it would require for its use. The
+	  control group is tracked in the third page lru pointer. This means
+	  that we cannot use the controller with huge page less than 3 pages.
+
+config CPUSETS
+	bool "Cpuset controller"
+	depends on SMP
+	help
+	  This option will let you create and manage CPUSETs which
+	  allow dynamically partitioning a system into sets of CPUs and
+	  Memory Nodes and assigning tasks to run only within those sets.
+	  This is primarily useful on large SMP or NUMA systems.
+
+	  Say N if unsure.
+
+config PROC_PID_CPUSET
+	bool "Include legacy /proc/<pid>/cpuset file"
+	depends on CPUSETS
+	default y
+
+config CGROUP_DEVICE
+	bool "Device controller"
+	help
+	  Provides a cgroup controller implementing whitelists for
+	  devices which a process in the cgroup can mknod or open.
+
+config CGROUP_CPUACCT
+	bool "Simple CPU accounting controller"
+	help
+	  Provides a simple controller for monitoring the
+	  total CPU consumed by the tasks in a cgroup.
+
+config CGROUP_PERF
+	bool "Perf controller"
+	depends on PERF_EVENTS
+	help
+	  This option extends the perf per-cpu mode to restrict monitoring
+	  to threads which belong to the cgroup specified and run on the
+	  designated cpu.  Or this can be used to have cgroup ID in samples
+	  so that it can monitor performance events among cgroups.
+
+	  Say N if unsure.
+
+config CGROUP_BPF
+	bool "Support for eBPF programs attached to cgroups"
+	depends on BPF_SYSCALL
+	select SOCK_CGROUP_DATA
+	help
+	  Allow attaching eBPF programs to a cgroup using the bpf(2)
+	  syscall command BPF_PROG_ATTACH.
+
+	  In which context these programs are accessed depends on the type
+	  of attachment. For instance, programs that are attached using
+	  BPF_CGROUP_INET_INGRESS will be executed on the ingress path of
+	  inet sockets.
+
+config CGROUP_MISC
+	bool "Misc resource controller"
+	default n
+	help
+	  Provides a controller for miscellaneous resources on a host.
+
+	  Miscellaneous scalar resources are the resources on the host system
+	  which cannot be abstracted like the other cgroups. This controller
+	  tracks and limits the miscellaneous resources used by a process
+	  attached to a cgroup hierarchy.
+
+	  For more information, please check misc cgroup section in
+	  /Documentation/admin-guide/cgroup-v2.rst.
+
+config CGROUP_DEBUG
+	bool "Debug controller"
+	default n
+	depends on DEBUG_KERNEL
+	help
+	  This option enables a simple controller that exports
+	  debugging information about the cgroups framework. This
+	  controller is for control cgroup debugging only. Its
+	  interfaces are not stable.
+
+	  Say N.
+
+config SOCK_CGROUP_DATA
+	bool
+	default n
+
+endif # CGROUPS
+
+menuconfig NAMESPACES
+	bool "Namespaces support" if EXPERT
+	depends on MULTIUSER
+	default !EXPERT
+	help
+	  Provides the way to make tasks work with different objects using
+	  the same id. For example same IPC id may refer to different objects
+	  or same user id or pid may refer to different tasks when used in
+	  different namespaces.
+
+if NAMESPACES
+
+config UTS_NS
+	bool "UTS namespace"
+	default y
+	help
+	  In this namespace tasks see different info provided with the
+	  uname() system call
+
+config TIME_NS
+	bool "TIME namespace"
+	depends on GENERIC_VDSO_TIME_NS
+	default y
+	help
+	  In this namespace boottime and monotonic clocks can be set.
+	  The time will keep going with the same pace.
+
+config IPC_NS
+	bool "IPC namespace"
+	depends on (SYSVIPC || POSIX_MQUEUE)
+	default y
+	help
+	  In this namespace tasks work with IPC ids which correspond to
+	  different IPC objects in different namespaces.
+
+config USER_NS
+	bool "User namespace"
+	default n
+	help
+	  This allows containers, i.e. vservers, to use user namespaces
+	  to provide different user info for different servers.
+
+	  When user namespaces are enabled in the kernel it is
+	  recommended that the MEMCG option also be enabled and that
+	  user-space use the memory control groups to limit the amount
+	  of memory a memory unprivileged users can use.
+
+	  If unsure, say N.
+
+config PID_NS
+	bool "PID Namespaces"
+	default y
+	help
+	  Support process id namespaces.  This allows having multiple
+	  processes with the same pid as long as they are in different
+	  pid namespaces.  This is a building block of containers.
+
+config NET_NS
+	bool "Network namespace"
+	depends on NET
+	default y
+	help
+	  Allow user space to create what appear to be multiple instances
+	  of the network stack.
+
+endif # NAMESPACES
+
+config CHECKPOINT_RESTORE
+	bool "Checkpoint/restore support"
+	select PROC_CHILDREN
+	select KCMP
+	default n
+	help
+	  Enables additional kernel features in a sake of checkpoint/restore.
+	  In particular it adds auxiliary prctl codes to setup process text,
+	  data and heap segment sizes, and a few additional /proc filesystem
+	  entries.
+
+	  If unsure, say N here.
+
+config SCHED_AUTOGROUP
+	bool "Automatic process group scheduling"
+	select CGROUPS
+	select CGROUP_SCHED
+	select FAIR_GROUP_SCHED
+	help
+	  This option optimizes the scheduler for common desktop workloads by
+	  automatically creating and populating task groups.  This separation
+	  of workloads isolates aggressive CPU burners (like build jobs) from
+	  desktop applications.  Task group autogeneration is currently based
+	  upon task session.
+
+config SYSFS_DEPRECATED
+	bool "Enable deprecated sysfs features to support old userspace tools"
+	depends on SYSFS
+	default n
+	help
+	  This option adds code that switches the layout of the "block" class
+	  devices, to not show up in /sys/class/block/, but only in
+	  /sys/block/.
+
+	  This switch is only active when the sysfs.deprecated=1 boot option is
+	  passed or the SYSFS_DEPRECATED_V2 option is set.
+
+	  This option allows new kernels to run on old distributions and tools,
+	  which might get confused by /sys/class/block/. Since 2007/2008 all
+	  major distributions and tools handle this just fine.
+
+	  Recent distributions and userspace tools after 2009/2010 depend on
+	  the existence of /sys/class/block/, and will not work with this
+	  option enabled.
+
+	  Only if you are using a new kernel on an old distribution, you might
+	  need to say Y here.
+
+config SYSFS_DEPRECATED_V2
+	bool "Enable deprecated sysfs features by default"
+	default n
+	depends on SYSFS
+	depends on SYSFS_DEPRECATED
+	help
+	  Enable deprecated sysfs by default.
+
+	  See the CONFIG_SYSFS_DEPRECATED option for more details about this
+	  option.
+
+	  Only if you are using a new kernel on an old distribution, you might
+	  need to say Y here. Even then, odds are you would not need it
+	  enabled, you can always pass the boot option if absolutely necessary.
+
+config RELAY
+	bool "Kernel->user space relay support (formerly relayfs)"
+	select IRQ_WORK
+	help
+	  This option enables support for relay interface support in
+	  certain file systems (such as debugfs).
+	  It is designed to provide an efficient mechanism for tools and
+	  facilities to relay large amounts of data from kernel space to
+	  user space.
+
+	  If unsure, say N.
+
+config BLK_DEV_INITRD
+	bool "Initial RAM filesystem and RAM disk (initramfs/initrd) support"
+	help
+	  The initial RAM filesystem is a ramfs which is loaded by the
+	  boot loader (loadlin or lilo) and that is mounted as root
+	  before the normal boot procedure. It is typically used to
+	  load modules needed to mount the "real" root file system,
+	  etc. See <file:Documentation/admin-guide/initrd.rst> for details.
+
+	  If RAM disk support (BLK_DEV_RAM) is also included, this
+	  also enables initial RAM disk (initrd) support and adds
+	  15 Kbytes (more on some other architectures) to the kernel size.
+
+	  If unsure say Y.
+
+if BLK_DEV_INITRD
+
+source "usr/Kconfig"
+
+endif
+
+config BOOT_CONFIG
+	bool "Boot config support"
+	select BLK_DEV_INITRD
+	help
+	  Extra boot config allows system admin to pass a config file as
+	  complemental extension of kernel cmdline when booting.
+	  The boot config file must be attached at the end of initramfs
+	  with checksum, size and magic word.
+	  See <file:Documentation/admin-guide/bootconfig.rst> for details.
+
+	  If unsure, say Y.
+
+choice
+	prompt "Compiler optimization level"
+	default CC_OPTIMIZE_FOR_PERFORMANCE
+
+config CC_OPTIMIZE_FOR_PERFORMANCE
+	bool "Optimize for performance (-O2)"
+	help
+	  This is the default optimization level for the kernel, building
+	  with the "-O2" compiler flag for best performance and most
+	  helpful compile-time warnings.
+
+config CC_OPTIMIZE_FOR_PERFORMANCE_O3
+	bool "Optimize more for performance (-O3)"
+	depends on ARC
+	help
+	  Choosing this option will pass "-O3" to your compiler to optimize
+	  the kernel yet more for performance.
+
+config CC_OPTIMIZE_FOR_SIZE
+	bool "Optimize for size (-Os)"
+	help
+	  Choosing this option will pass "-Os" to your compiler resulting
+	  in a smaller kernel.
+
+endchoice
+
+config HAVE_LD_DEAD_CODE_DATA_ELIMINATION
+	bool
+	help
+	  This requires that the arch annotates or otherwise protects
+	  its external entry points from being discarded. Linker scripts
+	  must also merge .text.*, .data.*, and .bss.* correctly into
+	  output sections. Care must be taken not to pull in unrelated
+	  sections (e.g., '.text.init'). Typically '.' in section names
+	  is used to distinguish them from label names / C identifiers.
+
+config LD_DEAD_CODE_DATA_ELIMINATION
+	bool "Dead code and data elimination (EXPERIMENTAL)"
+	depends on HAVE_LD_DEAD_CODE_DATA_ELIMINATION
+	depends on EXPERT
+	depends on $(cc-option,-ffunction-sections -fdata-sections)
+	depends on $(ld-option,--gc-sections)
+	help
+	  Enable this if you want to do dead code and data elimination with
+	  the linker by compiling with -ffunction-sections -fdata-sections,
+	  and linking with --gc-sections.
+
+	  This can reduce on disk and in-memory size of the kernel
+	  code and static data, particularly for small configs and
+	  on small systems. This has the possibility of introducing
+	  silently broken kernel if the required annotations are not
+	  present. This option is not well tested yet, so use at your
+	  own risk.
+
+config LD_ORPHAN_WARN
+	def_bool y
+	depends on ARCH_WANT_LD_ORPHAN_WARN
+	depends on !LD_IS_LLD || LLD_VERSION >= 110000
+	depends on $(ld-option,--orphan-handling=warn)
+
+config SYSCTL
+	bool
+
+config HAVE_UID16
+	bool
+
+config SYSCTL_EXCEPTION_TRACE
+	bool
+	help
+	  Enable support for /proc/sys/debug/exception-trace.
+
+config SYSCTL_ARCH_UNALIGN_NO_WARN
+	bool
+	help
+	  Enable support for /proc/sys/kernel/ignore-unaligned-usertrap
+	  Allows arch to define/use @no_unaligned_warning to possibly warn
+	  about unaligned access emulation going on under the hood.
+
+config SYSCTL_ARCH_UNALIGN_ALLOW
+	bool
+	help
+	  Enable support for /proc/sys/kernel/unaligned-trap
+	  Allows arches to define/use @unaligned_enabled to runtime toggle
+	  the unaligned access emulation.
+	  see arch/parisc/kernel/unaligned.c for reference
+
+config HAVE_PCSPKR_PLATFORM
+	bool
+
+# interpreter that classic socket filters depend on
+config BPF
+	bool
+
+menuconfig EXPERT
+	bool "Configure standard kernel features (expert users)"
+	# Unhide debug options, to make the on-by-default options visible
+	select DEBUG_KERNEL
+	help
+	  This option allows certain base kernel options and settings
+	  to be disabled or tweaked. This is for specialized
+	  environments which can tolerate a "non-standard" kernel.
+	  Only use this if you really know what you are doing.
+
+config UID16
+	bool "Enable 16-bit UID system calls" if EXPERT
+	depends on HAVE_UID16 && MULTIUSER
+	default y
+	help
+	  This enables the legacy 16-bit UID syscall wrappers.
+
+config MULTIUSER
+	bool "Multiple users, groups and capabilities support" if EXPERT
+	default y
+	help
+	  This option enables support for non-root users, groups and
+	  capabilities.
+
+	  If you say N here, all processes will run with UID 0, GID 0, and all
+	  possible capabilities.  Saying N here also compiles out support for
+	  system calls related to UIDs, GIDs, and capabilities, such as setuid,
+	  setgid, and capset.
+
+	  If unsure, say Y here.
+
+config SGETMASK_SYSCALL
+	bool "sgetmask/ssetmask syscalls support" if EXPERT
+	def_bool PARISC || M68K || PPC || MIPS || X86 || SPARC || MICROBLAZE || SUPERH
+	help
+	  sys_sgetmask and sys_ssetmask are obsolete system calls
+	  no longer supported in libc but still enabled by default in some
+	  architectures.
+
+	  If unsure, leave the default option here.
+
+config SYSFS_SYSCALL
+	bool "Sysfs syscall support" if EXPERT
+	default y
+	help
+	  sys_sysfs is an obsolete system call no longer supported in libc.
+	  Note that disabling this option is more secure but might break
+	  compatibility with some systems.
+
+	  If unsure say Y here.
+
+config FHANDLE
+	bool "open by fhandle syscalls" if EXPERT
+	select EXPORTFS
+	default y
+	help
+	  If you say Y here, a user level program will be able to map
+	  file names to handle and then later use the handle for
+	  different file system operations. This is useful in implementing
+	  userspace file servers, which now track files using handles instead
+	  of names. The handle would remain the same even if file names
+	  get renamed. Enables open_by_handle_at(2) and name_to_handle_at(2)
+	  syscalls.
+
+config POSIX_TIMERS
+	bool "Posix Clocks & timers" if EXPERT
+	default y
+	help
+	  This includes native support for POSIX timers to the kernel.
+	  Some embedded systems have no use for them and therefore they
+	  can be configured out to reduce the size of the kernel image.
+
+	  When this option is disabled, the following syscalls won't be
+	  available: timer_create, timer_gettime: timer_getoverrun,
+	  timer_settime, timer_delete, clock_adjtime, getitimer,
+	  setitimer, alarm. Furthermore, the clock_settime, clock_gettime,
+	  clock_getres and clock_nanosleep syscalls will be limited to
+	  CLOCK_REALTIME, CLOCK_MONOTONIC and CLOCK_BOOTTIME only.
+
+	  If unsure say y.
+
+config PRINTK
+	default y
+	bool "Enable support for printk" if EXPERT
+	select IRQ_WORK
+	help
+	  This option enables normal printk support. Removing it
+	  eliminates most of the message strings from the kernel image
+	  and makes the kernel more or less silent. As this makes it
+	  very difficult to diagnose system problems, saying N here is
+	  strongly discouraged.
+
+config BUG
+	bool "BUG() support" if EXPERT
+	default y
+	help
+	  Disabling this option eliminates support for BUG and WARN, reducing
+	  the size of your kernel image and potentially quietly ignoring
+	  numerous fatal conditions. You should only consider disabling this
+	  option for embedded systems with no facilities for reporting errors.
+	  Just say Y.
+
+config ELF_CORE
+	depends on COREDUMP
+	default y
+	bool "Enable ELF core dumps" if EXPERT
+	help
+	  Enable support for generating core dumps. Disabling saves about 4k.
+
+
+config PCSPKR_PLATFORM
+	bool "Enable PC-Speaker support" if EXPERT
+	depends on HAVE_PCSPKR_PLATFORM
+	select I8253_LOCK
+	default y
+	help
+	  This option allows to disable the internal PC-Speaker
+	  support, saving some memory.
+
+config BASE_FULL
+	default y
+	bool "Enable full-sized data structures for core" if EXPERT
+	help
+	  Disabling this option reduces the size of miscellaneous core
+	  kernel data structures. This saves memory on small machines,
+	  but may reduce performance.
+
+config FUTEX
+	bool "Enable futex support" if EXPERT
+	default y
+	imply RT_MUTEXES
+	help
+	  Disabling this option will cause the kernel to be built without
+	  support for "fast userspace mutexes".  The resulting kernel may not
+	  run glibc-based applications correctly.
+
+config FUTEX_PI
+	bool
+	depends on FUTEX && RT_MUTEXES
+	default y
+
+config HAVE_FUTEX_CMPXCHG
+	bool
+	depends on FUTEX
+	help
+	  Architectures should select this if futex_atomic_cmpxchg_inatomic()
+	  is implemented and always working. This removes a couple of runtime
+	  checks.
+
+config EPOLL
+	bool "Enable eventpoll support" if EXPERT
+	default y
+	help
+	  Disabling this option will cause the kernel to be built without
+	  support for epoll family of system calls.
+
+config SIGNALFD
+	bool "Enable signalfd() system call" if EXPERT
+	default y
+	help
+	  Enable the signalfd() system call that allows to receive signals
+	  on a file descriptor.
+
+	  If unsure, say Y.
+
+config TIMERFD
+	bool "Enable timerfd() system call" if EXPERT
+	default y
+	help
+	  Enable the timerfd() system call that allows to receive timer
+	  events on a file descriptor.
+
+	  If unsure, say Y.
+
+config EVENTFD
+	bool "Enable eventfd() system call" if EXPERT
+	default y
+	help
+	  Enable the eventfd() system call that allows to receive both
+	  kernel notification (ie. KAIO) or userspace notifications.
+
+	  If unsure, say Y.
+
+config SHMEM
+	bool "Use full shmem filesystem" if EXPERT
+	default y
+	depends on MMU
+	help
+	  The shmem is an internal filesystem used to manage shared memory.
+	  It is backed by swap and manages resource limits. It is also exported
+	  to userspace as tmpfs if TMPFS is enabled. Disabling this
+	  option replaces shmem and tmpfs with the much simpler ramfs code,
+	  which may be appropriate on small systems without swap.
+
+config AIO
+	bool "Enable AIO support" if EXPERT
+	default y
+	help
+	  This option enables POSIX asynchronous I/O which may by used
+	  by some high performance threaded applications. Disabling
+	  this option saves about 7k.
+
+config IO_URING
+	bool "Enable IO uring support" if EXPERT
+	select IO_WQ
+	default y
+	help
+	  This option enables support for the io_uring interface, enabling
+	  applications to submit and complete IO through submission and
+	  completion rings that are shared between the kernel and application.
+
+config ADVISE_SYSCALLS
+	bool "Enable madvise/fadvise syscalls" if EXPERT
+	default y
+	help
+	  This option enables the madvise and fadvise syscalls, used by
+	  applications to advise the kernel about their future memory or file
+	  usage, improving performance. If building an embedded system where no
+	  applications use these syscalls, you can disable this option to save
+	  space.
+
+config HAVE_ARCH_USERFAULTFD_WP
+	bool
+	help
+	  Arch has userfaultfd write protection support
+
+config HAVE_ARCH_USERFAULTFD_MINOR
+	bool
+	help
+	  Arch has userfaultfd minor fault support
+
+config MEMBARRIER
+	bool "Enable membarrier() system call" if EXPERT
+	default y
+	help
+	  Enable the membarrier() system call that allows issuing memory
+	  barriers across all running threads, which can be used to distribute
+	  the cost of user-space memory barriers asymmetrically by transforming
+	  pairs of memory barriers into pairs consisting of membarrier() and a
+	  compiler barrier.
+
+	  If unsure, say Y.
+
+config UMCG
+	bool "Enable User Managed Concurrency Groups API"
+	depends on 64BIT
+	depends on GOOGLE_SYSCALLS
+	default n
+	help
+	  Enable User Managed Concurrency Groups API, which form the basis
+	  for an in-process M:N userspace scheduling framework.
+	  At the moment this is an experimental/RFC feature that is not
+	  guaranteed to be backward-compatible.
+
+config UMCG_PREEMPT_JIFFIES
+	int "Interval at which running UMCG workers are preempted."
+	depends on UMCG
+	default 10
+	help
+	  Normally UMCG workers cooperatively "yield" their servers.
+	  However, UMCG worker preemption is needed in cases when
+	  the worker is waiting on a spinlock that a descheduled worker
+	  holds. In this case preempting a running worker gives the
+	  userspace scheduler a chance to run the lock-holding worker.
+
+config KALLSYMS
+	bool "Load all symbols for debugging/ksymoops" if EXPERT
+	default y
+	help
+	  Say Y here to let the kernel print out symbolic crash information and
+	  symbolic stack backtraces. This increases the size of the kernel
+	  somewhat, as all symbols have to be loaded into the kernel image.
+
+config KALLSYMS_ALL
+	bool "Include all symbols in kallsyms"
+	depends on DEBUG_KERNEL && KALLSYMS
+	help
+	  Normally kallsyms only contains the symbols of functions for nicer
+	  OOPS messages and backtraces (i.e., symbols from the text and inittext
+	  sections). This is sufficient for most cases. And only in very rare
+	  cases (e.g., when a debugger is used) all symbols are required (e.g.,
+	  names of variables from the data sections, etc).
+
+	  This option makes sure that all symbols are loaded into the kernel
+	  image (i.e., symbols from all sections) in cost of increased kernel
+	  size (depending on the kernel configuration, it may be 300KiB or
+	  something like this).
+
+	  Say N unless you really need all symbols.
+
+config KALLSYMS_ABSOLUTE_PERCPU
+	bool
+	depends on KALLSYMS
+	default X86_64 && SMP
+
+config KALLSYMS_BASE_RELATIVE
+	bool
+	depends on KALLSYMS
+	default !IA64
+	help
+	  Instead of emitting them as absolute values in the native word size,
+	  emit the symbol references in the kallsyms table as 32-bit entries,
+	  each containing a relative value in the range [base, base + U32_MAX]
+	  or, when KALLSYMS_ABSOLUTE_PERCPU is in effect, each containing either
+	  an absolute value in the range [0, S32_MAX] or a relative value in the
+	  range [base, base + S32_MAX], where base is the lowest relative symbol
+	  address encountered in the image.
+
+	  On 64-bit builds, this reduces the size of the address table by 50%,
+	  but more importantly, it results in entries whose values are build
+	  time constants, and no relocation pass is required at runtime to fix
+	  up the entries based on the runtime load address of the kernel.
+
+# end of the "standard kernel features (expert users)" menu
+
+# syscall, maps, verifier
+
+config USERFAULTFD
+	bool "Enable userfaultfd() system call"
+	depends on MMU
+	help
+	  Enable the userfaultfd() system call that allows to intercept and
+	  handle page faults in userland.
+
+config ARCH_HAS_MEMBARRIER_CALLBACKS
+	bool
+
+config ARCH_HAS_MEMBARRIER_SYNC_CORE
+	bool
+
+config KCMP
+	bool "Enable kcmp() system call" if EXPERT
+	help
+	  Enable the kernel resource comparison system call. It provides
+	  user-space with the ability to compare two processes to see if they
+	  share a common resource, such as a file descriptor or even virtual
+	  memory space.
+
+	  If unsure, say N.
+
+config RSEQ
+	bool "Enable rseq() system call" if EXPERT
+	default y
+	depends on HAVE_RSEQ
+	select MEMBARRIER
+	help
+	  Enable the restartable sequences system call. It provides a
+	  user-space cache for the current CPU number value, which
+	  speeds up getting the current CPU number from user-space,
+	  as well as an ABI to speed up user-space operations on
+	  per-CPU data.
+
+	  If unsure, say Y.
+
+config DEBUG_RSEQ
+	default n
+	bool "Enabled debugging of rseq() system call" if EXPERT
+	depends on RSEQ && DEBUG_KERNEL
+	help
+	  Enable extra debugging checks for the rseq system call.
+
+	  If unsure, say N.
+
+config EMBEDDED
+	bool "Embedded system"
+	select EXPERT
+	help
+	  This option should be enabled if compiling the kernel for
+	  an embedded system so certain expert options are available
+	  for configuration.
+
+config HAVE_PERF_EVENTS
+	bool
+	help
+	  See tools/perf/design.txt for details.
+
+config PERF_USE_VMALLOC
+	bool
+	help
+	  See tools/perf/design.txt for details
+
+config PC104
+	bool "PC/104 support" if EXPERT
+	help
+	  Expose PC/104 form factor device drivers and options available for
+	  selection and configuration. Enable this option if your target
+	  machine has a PC/104 bus.
+
+menu "Kernel Performance Events And Counters"
+
+config PERF_EVENTS
+	bool "Kernel performance events and counters"
+	default y if PROFILING
+	depends on HAVE_PERF_EVENTS
+	select IRQ_WORK
+	select SRCU
+	help
+	  Enable kernel support for various performance events provided
+	  by software and hardware.
+
+	  Software events are supported either built-in or via the
+	  use of generic tracepoints.
+
+	  Most modern CPUs support performance events via performance
+	  counter registers. These registers count the number of certain
+	  types of hw events: such as instructions executed, cachemisses
+	  suffered, or branches mis-predicted - without slowing down the
+	  kernel or applications. These registers can also trigger interrupts
+	  when a threshold number of events have passed - and can thus be
+	  used to profile the code that runs on that CPU.
+
+	  The Linux Performance Event subsystem provides an abstraction of
+	  these software and hardware event capabilities, available via a
+	  system call and used by the "perf" utility in tools/perf/. It
+	  provides per task and per CPU counters, and it provides event
+	  capabilities on top of those.
+
+	  Say Y if unsure.
+
+config DEBUG_PERF_USE_VMALLOC
+	default n
+	bool "Debug: use vmalloc to back perf mmap() buffers"
+	depends on PERF_EVENTS && DEBUG_KERNEL && !PPC
+	select PERF_USE_VMALLOC
+	help
+	  Use vmalloc memory to back perf mmap() buffers.
+
+	  Mostly useful for debugging the vmalloc code on platforms
+	  that don't require it.
+
+	  Say N if unsure.
+
+endmenu
+
+config VM_EVENT_COUNTERS
+	default y
+	bool "Enable VM event counters for /proc/vmstat" if EXPERT
+	help
+	  VM event counters are needed for event counts to be shown.
+	  This option allows the disabling of the VM event counters
+	  on EXPERT systems.  /proc/vmstat will only show page counts
+	  if VM event counters are disabled.
+
+config SLUB_DEBUG
+	default y
+	bool "Enable SLUB debugging support" if EXPERT
+	depends on SLUB && SYSFS
+	help
+	  SLUB has extensive debug support features. Disabling these can
+	  result in significant savings in code size. This also disables
+	  SLUB sysfs support. /sys/slab will not exist and there will be
+	  no support for cache validation etc.
+
+config COMPAT_BRK
+	bool "Disable heap randomization"
+	default y
+	help
+	  Randomizing heap placement makes heap exploits harder, but it
+	  also breaks ancient binaries (including anything libc5 based).
+	  This option changes the bootup default to heap randomization
+	  disabled, and can be overridden at runtime by setting
+	  /proc/sys/kernel/randomize_va_space to 2.
+
+	  On non-ancient distros (post-2000 ones) N is usually a safe choice.
+
+choice
+	prompt "Choose SLAB allocator"
+	default SLUB
+	help
+	   This option allows to select a slab allocator.
+
+config SLAB
+	bool "SLAB"
+	select HAVE_HARDENED_USERCOPY_ALLOCATOR
+	help
+	  The regular slab allocator that is established and known to work
+	  well in all environments. It organizes cache hot objects in
+	  per cpu and per node queues.
+
+config SLUB
+	bool "SLUB (Unqueued Allocator)"
+	select HAVE_HARDENED_USERCOPY_ALLOCATOR
+	help
+	   SLUB is a slab allocator that minimizes cache line usage
+	   instead of managing queues of cached objects (SLAB approach).
+	   Per cpu caching is realized using slabs of objects instead
+	   of queues of objects. SLUB can use memory efficiently
+	   and has enhanced diagnostics. SLUB is the default choice for
+	   a slab allocator.
+
+config SLOB
+	depends on EXPERT
+	bool "SLOB (Simple Allocator)"
+	help
+	   SLOB replaces the stock allocator with a drastically simpler
+	   allocator. SLOB is generally more space efficient but
+	   does not perform as well on large systems.
+
+endchoice
+
+config SLAB_MERGE_DEFAULT
+	bool "Allow slab caches to be merged"
+	default y
+	help
+	  For reduced kernel memory fragmentation, slab caches can be
+	  merged when they share the same size and other characteristics.
+	  This carries a risk of kernel heap overflows being able to
+	  overwrite objects from merged caches (and more easily control
+	  cache layout), which makes such heap attacks easier to exploit
+	  by attackers. By keeping caches unmerged, these kinds of exploits
+	  can usually only damage objects in the same cache. To disable
+	  merging at runtime, "slab_nomerge" can be passed on the kernel
+	  command line.
+
+config SLAB_FREELIST_RANDOM
+	bool "Randomize slab freelist"
+	depends on SLAB || SLUB
+	help
+	  Randomizes the freelist order used on creating new pages. This
+	  security feature reduces the predictability of the kernel slab
+	  allocator against heap overflows.
+
+config SLAB_FREELIST_HARDENED
+	bool "Harden slab freelist metadata"
+	depends on SLAB || SLUB
+	help
+	  Many kernel heap attacks try to target slab cache metadata and
+	  other infrastructure. This options makes minor performance
+	  sacrifices to harden the kernel slab allocator against common
+	  freelist exploit methods. Some slab implementations have more
+	  sanity-checking than others. This option is most effective with
+	  CONFIG_SLUB.
+
+config SHUFFLE_PAGE_ALLOCATOR
+	bool "Page allocator randomization"
+	default SLAB_FREELIST_RANDOM && ACPI_NUMA
+	help
+	  Randomization of the page allocator improves the average
+	  utilization of a direct-mapped memory-side-cache. See section
+	  5.2.27 Heterogeneous Memory Attribute Table (HMAT) in the ACPI
+	  6.2a specification for an example of how a platform advertises
+	  the presence of a memory-side-cache. There are also incidental
+	  security benefits as it reduces the predictability of page
+	  allocations to compliment SLAB_FREELIST_RANDOM, but the
+	  default granularity of shuffling on the "MAX_ORDER - 1" i.e,
+	  10th order of pages is selected based on cache utilization
+	  benefits on x86.
+
+	  While the randomization improves cache utilization it may
+	  negatively impact workloads on platforms without a cache. For
+	  this reason, by default, the randomization is enabled only
+	  after runtime detection of a direct-mapped memory-side-cache.
+	  Otherwise, the randomization may be force enabled with the
+	  'page_alloc.shuffle' kernel command line parameter.
+
+	  Say Y if unsure.
+
+config SLUB_CPU_PARTIAL
+	default y
+	depends on SLUB && SMP
+	bool "SLUB per cpu partial cache"
+	help
+	  Per cpu partial caches accelerate objects allocation and freeing
+	  that is local to a processor at the price of more indeterminism
+	  in the latency of the free. On overflow these caches will be cleared
+	  which requires the taking of locks that may cause latency spikes.
+	  Typically one would choose no for a realtime system.
+
+config MMAP_ALLOW_UNINITIALIZED
+	bool "Allow mmapped anonymous memory to be uninitialized"
+	depends on EXPERT && !MMU
+	default n
+	help
+	  Normally, and according to the Linux spec, anonymous memory obtained
+	  from mmap() has its contents cleared before it is passed to
+	  userspace.  Enabling this config option allows you to request that
+	  mmap() skip that if it is given an MAP_UNINITIALIZED flag, thus
+	  providing a huge performance boost.  If this option is not enabled,
+	  then the flag will be ignored.
+
+	  This is taken advantage of by uClibc's malloc(), and also by
+	  ELF-FDPIC binfmt's brk and stack allocator.
+
+	  Because of the obvious security issues, this option should only be
+	  enabled on embedded devices where you control what is run in
+	  userspace.  Since that isn't generally a problem on no-MMU systems,
+	  it is normally safe to say Y here.
+
+	  See Documentation/admin-guide/mm/nommu-mmap.rst for more information.
+
+config SYSTEM_DATA_VERIFICATION
+	def_bool n
+	select SYSTEM_TRUSTED_KEYRING
+	select KEYS
+	select CRYPTO
+	select CRYPTO_RSA
+	select ASYMMETRIC_KEY_TYPE
+	select ASYMMETRIC_PUBLIC_KEY_SUBTYPE
+	select ASN1
+	select OID_REGISTRY
+	select X509_CERTIFICATE_PARSER
+	select PKCS7_MESSAGE_PARSER
+	help
+	  Provide PKCS#7 message verification using the contents of the system
+	  trusted keyring to provide public keys.  This then can be used for
+	  module verification, kexec image verification and firmware blob
+	  verification.
+
+config PROFILING
+	bool "Profiling support"
+	help
+	  Say Y here to enable the extended profiling support mechanisms used
+	  by profilers.
+
+#
+# Place an empty function call at each tracepoint site. Can be
+# dynamically changed for a probe function.
+#
+config TRACEPOINTS
+	bool
+
+endmenu		# General setup
+
+source "arch/Kconfig"
+
+config RT_MUTEXES
+	bool
+
+config BASE_SMALL
+	int
+	default 0 if BASE_FULL
+	default 1 if !BASE_FULL
+
+config MODULE_SIG_FORMAT
+	def_bool n
+	select SYSTEM_DATA_VERIFICATION
+
+menuconfig MODULES
+	bool "Enable loadable module support"
+	modules
+	help
+	  Kernel modules are small pieces of compiled code which can
+	  be inserted in the running kernel, rather than being
+	  permanently built into the kernel.  You use the "modprobe"
+	  tool to add (and sometimes remove) them.  If you say Y here,
+	  many parts of the kernel can be built as modules (by
+	  answering M instead of Y where indicated): this is most
+	  useful for infrequently used options which are not required
+	  for booting.  For more information, see the man pages for
+	  modprobe, lsmod, modinfo, insmod and rmmod.
+
+	  If you say Y here, you will need to run "make
+	  modules_install" to put the modules under /lib/modules/
+	  where modprobe can find them (you may need to be root to do
+	  this).
+
+	  If unsure, say Y.
+
+if MODULES
+
+config MODULE_FORCE_LOAD
+	bool "Forced module loading"
+	default n
+	help
+	  Allow loading of modules without version information (ie. modprobe
+	  --force).  Forced module loading sets the 'F' (forced) taint flag and
+	  is usually a really bad idea.
+
+config MODULE_UNLOAD
+	bool "Module unloading"
+	help
+	  Without this option you will not be able to unload any
+	  modules (note that some modules may not be unloadable
+	  anyway), which makes your kernel smaller, faster
+	  and simpler.  If unsure, say Y.
+
+config MODULE_FORCE_UNLOAD
+	bool "Forced module unloading"
+	depends on MODULE_UNLOAD
+	help
+	  This option allows you to force a module to unload, even if the
+	  kernel believes it is unsafe: the kernel will remove the module
+	  without waiting for anyone to stop using it (using the -f option to
+	  rmmod).  This is mainly for kernel developers and desperate users.
+	  If unsure, say N.
+
+config MODVERSIONS
+	bool "Module versioning support"
+	help
+	  Usually, you have to use modules compiled with your kernel.
+	  Saying Y here makes it sometimes possible to use modules
+	  compiled for different kernels, by adding enough information
+	  to the modules to (hopefully) spot any changes which would
+	  make them incompatible with the kernel you are running.  If
+	  unsure, say N.
+
+config ASM_MODVERSIONS
+	bool
+	default HAVE_ASM_MODVERSIONS && MODVERSIONS
+	help
+	  This enables module versioning for exported symbols also from
+	  assembly. This can be enabled only when the target architecture
+	  supports it.
+
+config MODULE_REL_CRCS
+	bool
+	depends on MODVERSIONS
+
+config MODULE_SRCVERSION_ALL
+	bool "Source checksum for all modules"
+	help
+	  Modules which contain a MODULE_VERSION get an extra "srcversion"
+	  field inserted into their modinfo section, which contains a
+    	  sum of the source files which made it.  This helps maintainers
+	  see exactly which source was used to build a module (since
+	  others sometimes change the module source without updating
+	  the version).  With this option, such a "srcversion" field
+	  will be created for all modules.  If unsure, say N.
+
+config MODULE_SIG
+	bool "Module signature verification"
+	select MODULE_SIG_FORMAT
+	help
+	  Check modules for valid signatures upon load: the signature
+	  is simply appended to the module. For more information see
+	  <file:Documentation/admin-guide/module-signing.rst>.
+
+	  Note that this option adds the OpenSSL development packages as a
+	  kernel build dependency so that the signing tool can use its crypto
+	  library.
+
+	  You should enable this option if you wish to use either
+	  CONFIG_SECURITY_LOCKDOWN_LSM or lockdown functionality imposed via
+	  another LSM - otherwise unsigned modules will be loadable regardless
+	  of the lockdown policy.
+
+	  !!!WARNING!!!  If you enable this option, you MUST make sure that the
+	  module DOES NOT get stripped after being signed.  This includes the
+	  debuginfo strip done by some packagers (such as rpmbuild) and
+	  inclusion into an initramfs that wants the module size reduced.
+
+config MODULE_SIG_FORCE
+	bool "Require modules to be validly signed"
+	depends on MODULE_SIG
+	help
+	  Reject unsigned modules or signed modules for which we don't have a
+	  key.  Without this, such modules will simply taint the kernel.
+
+config MODULE_SIG_ALL
+	bool "Automatically sign all modules"
+	default y
+	depends on MODULE_SIG || IMA_APPRAISE_MODSIG
+	help
+	  Sign all modules during make modules_install. Without this option,
+	  modules must be signed manually, using the scripts/sign-file tool.
+
+comment "Do not forget to sign required modules with scripts/sign-file"
+	depends on MODULE_SIG_FORCE && !MODULE_SIG_ALL
+
+choice
+	prompt "Which hash algorithm should modules be signed with?"
+	depends on MODULE_SIG || IMA_APPRAISE_MODSIG
+	help
+	  This determines which sort of hashing algorithm will be used during
+	  signature generation.  This algorithm _must_ be built into the kernel
+	  directly so that signature verification can take place.  It is not
+	  possible to load a signed module containing the algorithm to check
+	  the signature on that module.
+
+config MODULE_SIG_SHA1
+	bool "Sign modules with SHA-1"
+	select CRYPTO_SHA1
+
+config MODULE_SIG_SHA224
+	bool "Sign modules with SHA-224"
+	select CRYPTO_SHA256
+
+config MODULE_SIG_SHA256
+	bool "Sign modules with SHA-256"
+	select CRYPTO_SHA256
+
+config MODULE_SIG_SHA384
+	bool "Sign modules with SHA-384"
+	select CRYPTO_SHA512
+
+config MODULE_SIG_SHA512
+	bool "Sign modules with SHA-512"
+	select CRYPTO_SHA512
+
+endchoice
+
+config MODULE_SIG_HASH
+	string
+	depends on MODULE_SIG || IMA_APPRAISE_MODSIG
+	default "sha1" if MODULE_SIG_SHA1
+	default "sha224" if MODULE_SIG_SHA224
+	default "sha256" if MODULE_SIG_SHA256
+	default "sha384" if MODULE_SIG_SHA384
+	default "sha512" if MODULE_SIG_SHA512
+
+choice
+	prompt "Module compression mode"
+	help
+	  This option allows you to choose the algorithm which will be used to
+	  compress modules when 'make modules_install' is run. (or, you can
+	  choose to not compress modules at all.)
+
+	  External modules will also be compressed in the same way during the
+	  installation.
+
+	  For modules inside an initrd or initramfs, it's more efficient to
+	  compress the whole initrd or initramfs instead.
+
+	  This is fully compatible with signed modules.
+
+	  Please note that the tool used to load modules needs to support the
+	  corresponding algorithm. module-init-tools MAY support gzip, and kmod
+	  MAY support gzip, xz and zstd.
+
+	  Your build system needs to provide the appropriate compression tool
+	  to compress the modules.
+
+	  If in doubt, select 'None'.
+
+config MODULE_COMPRESS_NONE
+	bool "None"
+	help
+	  Do not compress modules. The installed modules are suffixed
+	  with .ko.
+
+config MODULE_COMPRESS_GZIP
+	bool "GZIP"
+	help
+	  Compress modules with GZIP. The installed modules are suffixed
+	  with .ko.gz.
+
+config MODULE_COMPRESS_XZ
+	bool "XZ"
+	help
+	  Compress modules with XZ. The installed modules are suffixed
+	  with .ko.xz.
+
+config MODULE_COMPRESS_ZSTD
+	bool "ZSTD"
+	help
+	  Compress modules with ZSTD. The installed modules are suffixed
+	  with .ko.zst.
+
+endchoice
+
+config MODULE_ALLOW_MISSING_NAMESPACE_IMPORTS
+	bool "Allow loading of modules with missing namespace imports"
+	help
+	  Symbols exported with EXPORT_SYMBOL_NS*() are considered exported in
+	  a namespace. A module that makes use of a symbol exported with such a
+	  namespace is required to import the namespace via MODULE_IMPORT_NS().
+	  There is no technical reason to enforce correct namespace imports,
+	  but it creates consistency between symbols defining namespaces and
+	  users importing namespaces they make use of. This option relaxes this
+	  requirement and lifts the enforcement when loading a module.
+
+	  If unsure, say N.
+
+config MODPROBE_PATH
+	string "Path to modprobe binary"
+	default "/sbin/modprobe"
+	help
+	  When kernel code requests a module, it does so by calling
+	  the "modprobe" userspace utility. This option allows you to
+	  set the path where that binary is found. This can be changed
+	  at runtime via the sysctl file
+	  /proc/sys/kernel/modprobe. Setting this to the empty string
+	  removes the kernel's ability to request modules (but
+	  userspace can still load modules explicitly).
+
+config TRIM_UNUSED_KSYMS
+	bool "Trim unused exported kernel symbols" if EXPERT
+	depends on !COMPILE_TEST
+	help
+	  The kernel and some modules make many symbols available for
+	  other modules to use via EXPORT_SYMBOL() and variants. Depending
+	  on the set of modules being selected in your kernel configuration,
+	  many of those exported symbols might never be used.
+
+	  This option allows for unused exported symbols to be dropped from
+	  the build. In turn, this provides the compiler more opportunities
+	  (especially when using LTO) for optimizing the code and reducing
+	  binary size.  This might have some security advantages as well.
+
+	  If unsure, or if you need to build out-of-tree modules, say N.
+
+config UNUSED_KSYMS_WHITELIST
+	string "Whitelist of symbols to keep in ksymtab"
+	depends on TRIM_UNUSED_KSYMS
+	help
+	  By default, all unused exported symbols will be un-exported from the
+	  build when TRIM_UNUSED_KSYMS is selected.
+
+	  UNUSED_KSYMS_WHITELIST allows to whitelist symbols that must be kept
+	  exported at all times, even in absence of in-tree users. The value to
+	  set here is the path to a text file containing the list of symbols,
+	  one per line. The path can be absolute, or relative to the kernel
+	  source tree.
+
+endif # MODULES
+
+config MODULES_TREE_LOOKUP
+	def_bool y
+	depends on PERF_EVENTS || TRACING || CFI_CLANG
+
+config INIT_ALL_POSSIBLE
+	bool
+	help
+	  Back when each arch used to define their own cpu_online_mask and
+	  cpu_possible_mask, some of them chose to initialize cpu_possible_mask
+	  with all 1s, and others with all 0s.  When they were centralised,
+	  it was better to provide this option than to break all the archs
+	  and have several arch maintainers pursuing me down dark alleys.
+
+source "block/Kconfig"
+
+config PREEMPT_NOTIFIERS
+	bool
+
+config PADATA
+	depends on SMP
+	bool
+
+config ASN1
+	tristate
+	help
+	  Build a simple ASN.1 grammar compiler that produces a bytecode output
+	  that can be interpreted by the ASN.1 stream decoder and used to
+	  inform it as to what tags are to be expected in a stream and what
+	  functions to call on what tags.
+
+source "kernel/Kconfig.locks"
+
+config ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE
+	bool
+
+config ARCH_HAS_SYNC_CORE_BEFORE_USERMODE
+	bool
+
+# It may be useful for an architecture to override the definitions of the
+# SYSCALL_DEFINE() and __SYSCALL_DEFINEx() macros in <linux/syscalls.h>
+# and the COMPAT_ variants in <linux/compat.h>, in particular to use a
+# different calling convention for syscalls. They can also override the
+# macros for not-implemented syscalls in kernel/sys_ni.c and
+# kernel/time/posix-stubs.c. All these overrides need to be available in
+# <asm/syscall_wrapper.h>.
+config ARCH_HAS_SYSCALL_WRAPPER
+	def_bool n
diff --git a/kernel/exit.c b/kernel/exit.c
index 91a43e57a..4c93a16aa 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -743,6 +743,8 @@ void __noreturn do_exit(long code)
 	if (unlikely(!tsk->pid))
 		panic("Attempted to kill the idle task!");
 
+	umcg_handle_exit();
+
 	/*
 	 * If do_exit is called because this processes oopsed, it's possible
 	 * that get_fs() was left as KERNEL_DS, so reset it to USER_DS before
diff --git a/kernel/fork.c b/kernel/fork.c
index 38681ad44..57a6b8ed5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1061,6 +1061,11 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
+#endif
+#ifdef CONFIG_UMCG
+	INIT_LIST_HEAD(&mm->umcg_idle_server_list);
+	mm->umcg_idle_workers = NULL;
+	spin_lock_init(&mm->umcg_lock);
 #endif
 	mm_init_uprobes_state(mm);
 	hugetlb_count_init(mm);
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 978fcfca5..e4e481eee 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -37,3 +37,4 @@ obj-$(CONFIG_MEMBARRIER) += membarrier.o
 obj-$(CONFIG_CPU_ISOLATION) += isolation.o
 obj-$(CONFIG_PSI) += psi.o
 obj-$(CONFIG_SCHED_CORE) += core_sched.o
+obj-$(CONFIG_UMCG) += umcg.o
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f21714ea3..39cfc5c43 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3944,8 +3944,7 @@ bool ttwu_state_match(struct task_struct *p, unsigned int state, int *success)
  * Return: %true if @p->state changes (an actual wakeup was done),
  *	   %false otherwise.
  */
-static int
-try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
+int try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 {
 	unsigned long flags;
 	int cpu, success = 0;
@@ -4222,6 +4221,12 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->wake_entry.u_flags = CSD_TYPE_TTWU;
 	p->migration_pending = NULL;
 #endif
+#ifdef CONFIG_UMCG
+	p->umcg_worker_id = 0;
+	p->umcg_server = NULL;
+	INIT_LIST_HEAD(&p->umcg_idle_server_list);
+	atomic_long_set(&p->umcg_status, 0);
+#endif
 }
 
 DEFINE_STATIC_KEY_FALSE(sched_numa_balancing);
@@ -5219,6 +5224,7 @@ void scheduler_tick(void)
 		resched_latency_warn(cpu, resched_latency);
 
 	perf_event_task_tick();
+	umcg_tick(curr);
 
 #ifdef CONFIG_SMP
 	rq->idle_balance = idle_cpu(cpu);
@@ -6313,11 +6319,13 @@ void __noreturn do_task_dead(void)
 static inline void sched_submit_work(struct task_struct *tsk)
 {
 	unsigned int task_flags;
+	bool umcg_worker;
 
 	if (task_is_running(tsk))
 		return;
 
 	task_flags = tsk->flags;
+	umcg_worker = umcg_wq_work(tsk);
 	/*
 	 * If a worker went to sleep, notify and ask workqueue whether
 	 * it wants to wake up a task to maintain concurrency.
@@ -6326,12 +6334,15 @@ static inline void sched_submit_work(struct task_struct *tsk)
 	 * in the possible wakeup of a kworker and because wq_worker_sleeping()
 	 * requires it.
 	 */
-	if (task_flags & (PF_WQ_WORKER | PF_IO_WORKER)) {
+	if ((task_flags & (PF_WQ_WORKER | PF_IO_WORKER)) || umcg_worker) {
 		preempt_disable();
 		if (task_flags & PF_WQ_WORKER)
 			wq_worker_sleeping(tsk);
-		else
+		else if (task_flags & PF_IO_WORKER)
 			io_wq_worker_sleeping(tsk);
+
+		if (umcg_worker)
+			umcg_wq_worker_sleeping(tsk);
 		preempt_enable_no_resched();
 	}
 
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3d3e5793e..1a14f9978 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2048,6 +2048,7 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 #define WF_SYNC     0x10 /* Waker goes to sleep after wakeup */
 #define WF_MIGRATED 0x20 /* Internal use, task got migrated */
 #define WF_ON_CPU   0x40 /* Wakee is on_cpu */
+#define WF_CURRENT_CPU  0x80 /* Prefer to move the wakee to the current CPU. */
 
 #ifdef CONFIG_SMP
 static_assert(WF_EXEC == SD_BALANCE_EXEC);
@@ -3058,6 +3059,8 @@ static inline bool is_per_cpu_kthread(struct task_struct *p)
 extern void swake_up_all_locked(struct swait_queue_head *q);
 extern void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait);
 
+extern int try_to_wake_up(struct task_struct *tsk, unsigned int state, int wake_flags);
+
 #ifdef CONFIG_PREEMPT_DYNAMIC
 extern int preempt_dynamic_mode;
 extern int sched_dynamic_mode(const char *str);
diff --git a/kernel/sched/umcg.c b/kernel/sched/umcg.c
new file mode 100644
index 000000000..9eb3b9d9f
--- /dev/null
+++ b/kernel/sched/umcg.c
@@ -0,0 +1,1106 @@
+// SPDX-License-Identifier: GPL-2.0-only
+
+/*
+ * User Managed Concurrency Groups (UMCG).
+ */
+
+#include <linux/sched.h>
+#include <linux/syscalls.h>
+#include <linux/types.h>
+#include <linux/umcg.h>
+#include "sched.h"
+
+/* UMCG status values and flags. */
+enum {
+	UMCG_SERVER_ATTACHED	= 1UL,       /* Attached to a running worker. */
+	UMCG_SERVER_IDLE	= 2UL,       /* Nothing to do.                */
+	UMCG_SERVER_RUNNING	= 4UL,       /* Running on a CPU.             */
+	UMCG_SERVER_WAIT	= 8UL,       /* Waiting for a blocked worker. */
+
+	UMCG_WORKER_BLOCKED	= 0x010UL,   /* Off idle workers queue.  */
+	UMCG_WORKER_WAKING	= 0x020UL,   /* On idle workers queue.   */
+	UMCG_WORKER_SWAPPING	= 0x040UL,   /* On idle workers queue.   */
+	UMCG_WORKER_RUNNABLE	= 0x080UL,   /* Off idle workers queue.  */
+	UMCG_WORKER_RUNNING	= 0x100UL,   /* Running with a server attached. */
+
+	/* Can be ORed with SWAPPING and RUNNABLE.  */
+	UMCG_WORKER_TIMEOUT	= 0x200UL,   /* On idle workers queue.   */
+};
+
+#define UMCG_STATUS_RUNNING_MASK   (UMCG_SERVER_RUNNING | UMCG_WORKER_RUNNING)
+
+#define UMCG_WORKER_MASK \
+	(UMCG_WORKER_BLOCKED | UMCG_WORKER_WAKING | UMCG_WORKER_SWAPPING | UMCG_WORKER_RUNNABLE | UMCG_WORKER_RUNNING)
+
+#define UMCG_WARN(format, ...) \
+	pr_warn("%s:%d " format "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+#define UMCG_WQ_DISABLE_BIT	(1UL)
+
+static bool umcg_worker(struct task_struct *tsk)
+{
+	return atomic_long_read_acquire(&tsk->umcg_status) & UMCG_WORKER_MASK;
+}
+
+bool umcg_wq_work(struct task_struct *curr)
+{
+	return (atomic_long_read_acquire(&curr->umcg_status) == UMCG_WORKER_RUNNING) &&
+		!(curr->umcg_worker_id & UMCG_WQ_DISABLE_BIT);
+}
+
+static void umcg_wq_work_disable(struct task_struct *curr)
+{
+	curr->umcg_worker_id |= UMCG_WQ_DISABLE_BIT;
+}
+
+static void umcg_wq_work_enable(struct task_struct *curr)
+{
+	curr->umcg_worker_id &= ~UMCG_WQ_DISABLE_BIT;
+}
+
+void umcg_execve(struct task_struct *tsk)
+{
+	struct task_struct *curr = current;
+
+	curr->umcg_server = NULL;
+	curr->umcg_worker_id = 0;
+	atomic_long_set_release(&curr->umcg_status, 0);
+}
+
+static void umcg_wake_server(struct task_struct *server, u64 umcg_worker_id,
+				enum umcg_event_type worker_event)
+{
+	u64 curr_status;
+
+	if (WARN_ON(!server))
+		goto kill;
+
+	/*
+	 * Note that umcg_wake_server() might be called from preempt-disabled
+	 * context, so here we just update the in-kernel event field and let
+	 * umcg_server_idle_loop deal with the userspace events buffer.
+	 */
+	if (atomic_long_xchg(&server->umcg_worker_event, umcg_worker_id | worker_event)) {
+		UMCG_WARN("bad event");
+		goto kill;
+	}
+
+	curr_status = atomic_long_cmpxchg(&server->umcg_status,
+				UMCG_SERVER_ATTACHED, UMCG_SERVER_RUNNING);
+
+	if (curr_status != UMCG_SERVER_ATTACHED) {
+		if (curr_status == 0)
+			return;  /* Exiting/killed. */
+		UMCG_WARN("bad status: 0x%llx", curr_status);
+		goto kill;
+	}
+
+	try_to_wake_up(server, TASK_NORMAL,
+			worker_event == (u64)UMCG_WE_WAIT ? WF_CURRENT_CPU : 0);
+	return;
+
+kill:
+	force_sig(SIGKILL);
+}
+
+static void umcg_enqueue_idle_worker(struct task_struct *worker)
+{
+	bool kill = false;
+
+	spin_lock(&worker->mm->umcg_lock);
+	if (unlikely(worker->umcg_next_idle_worker)) {
+		UMCG_WARN("task on the list; worker: %p", worker);
+		kill = true;
+	} else {
+		worker->umcg_next_idle_worker = worker->mm->umcg_idle_workers;
+		worker->mm->umcg_idle_workers = worker;
+	}
+	spin_unlock(&worker->mm->umcg_lock);
+
+	if (kill)
+		force_sig(SIGKILL);
+}
+
+static struct task_struct *umcg_dequeue_idle_worker(void)
+{
+	struct task_struct *curr = current;
+	struct task_struct *worker;
+
+	spin_lock(&curr->mm->umcg_lock);
+	worker = curr->mm->umcg_idle_workers;
+	if (worker) {
+		curr->mm->umcg_idle_workers =
+			READ_ONCE(worker->umcg_next_idle_worker);
+		WRITE_ONCE(worker->umcg_next_idle_worker, NULL);
+	}
+	spin_unlock(&curr->mm->umcg_lock);
+
+	return worker;
+}
+
+static void umcg_enqueue_idle_server(void)
+{
+	struct task_struct *server = current;
+	bool kill = false;
+
+	spin_lock(&server->mm->umcg_lock);
+	if (server->mm->umcg_idle_workers) {
+		/* Idle workers present, don't enqueue. */
+		u64 umcg_status = atomic_long_cmpxchg(&server->umcg_status,
+				UMCG_SERVER_IDLE, UMCG_SERVER_RUNNING);
+
+		if (WARN_ONCE(umcg_status != UMCG_SERVER_IDLE,
+				"Unexpected server status: 0x%llx\n", umcg_status)) {
+			kill = true;
+		}
+	} else if (unlikely(!list_empty(&server->umcg_idle_server_list))) {
+		UMCG_WARN("server on the list; server: %p\n", server);
+		kill = true;
+	} else {
+		list_add(&server->umcg_idle_server_list,
+				&server->mm->umcg_idle_server_list);
+	}
+	spin_unlock(&server->mm->umcg_lock);
+
+	if (kill)
+		force_sig(SIGKILL);
+}
+
+static bool umcg_wake_idle_server(void)
+{
+	struct task_struct *curr = current;
+	struct task_struct *server;
+
+	spin_lock(&curr->mm->umcg_lock);
+	server = list_first_entry_or_null(&curr->mm->umcg_idle_server_list,
+			struct task_struct, umcg_idle_server_list);
+	if (server)
+		list_del_init(&server->umcg_idle_server_list);
+	spin_unlock(&curr->mm->umcg_lock);
+
+	if (!server)
+		return false;
+
+	atomic_long_set_release(&server->umcg_status, UMCG_SERVER_RUNNING);
+
+	try_to_wake_up(server, TASK_NORMAL, 0);
+	return true;
+}
+
+static long umcg_register_worker(u64 flags, pid_t next_tid, u64 umcg_worker_id,
+					u64 __user *events, int event_sz)
+{
+	struct task_struct *curr = current;
+
+	if (flags || next_tid || (u64)events || event_sz)
+		return -EINVAL;
+
+	if (atomic_long_read_acquire(&curr->umcg_status))
+		return -EINVAL;
+
+	/* Must have the last 5 bits as zero. */
+	if (umcg_worker_id & (UMCG_WORKER_ID_ALIGNMENT - 1))
+		return -EINVAL;
+
+	WRITE_ONCE(curr->umcg_server, NULL);
+	WRITE_ONCE(curr->umcg_worker_id, umcg_worker_id);
+	WRITE_ONCE(curr->umcg_next_idle_worker, NULL);
+
+	atomic_long_set_release(&curr->umcg_status, UMCG_WORKER_BLOCKED);
+
+	/* Trigger UMCG_WE_WAKE. */
+	set_tsk_thread_flag(curr, TIF_NOTIFY_RESUME);
+	return 0;
+}
+
+static long umcg_register_server(u64 flags, pid_t next_tid, u64 abs_timeout,
+					u64 __user *events, int event_sz)
+{
+	struct task_struct *curr = current;
+
+	if (flags || next_tid || (u64)events || event_sz || abs_timeout)
+		return -EINVAL;
+
+	if (atomic_long_read_acquire(&curr->umcg_status))
+		return -EINVAL;
+
+	INIT_LIST_HEAD(&curr->umcg_idle_server_list);
+	atomic_long_set_release(&curr->umcg_worker_event, 0);
+	atomic_long_set_release(&curr->umcg_status, UMCG_SERVER_RUNNING);
+
+	/* Non-blocking: return immediately. */
+	return 0;
+}
+
+static long umcg_unregister(u64 flags, pid_t next_tid, u64 abs_timeout,
+				u64 __user *events, int event_sz)
+{
+	struct task_struct *curr = current;
+	struct task_struct *server;
+	u64 umcg_status = atomic_long_read_acquire(&curr->umcg_status);
+
+	if (fatal_signal_pending(curr)) {
+		/* Do an unconditional cleanup. */
+		if (umcg_worker(curr)) {
+			server = READ_ONCE(curr->umcg_server);
+			if (server)
+				put_task_struct(server);
+		}
+		atomic_long_set_release(&curr->umcg_status, 0);
+
+		return 0;
+	}
+
+	if (flags || next_tid || (u64)events || event_sz || abs_timeout || !umcg_status)
+		return -EINVAL;
+
+	if (umcg_status == UMCG_SERVER_RUNNING) {
+		atomic_long_set_release(&curr->umcg_status, 0);
+		return 0;
+	}
+
+	if (umcg_status != UMCG_WORKER_RUNNING)
+		return -EINVAL;
+
+	atomic_long_set_release(&curr->umcg_status, 0);
+	server = xchg(&curr->umcg_server, NULL);
+	WRITE_ONCE(curr->umcg_next_idle_worker, NULL);
+	if (WARN_ON(!server)) {
+		force_sig(SIGKILL);
+		return -EINVAL;
+	}
+
+	umcg_wake_server(server, curr->umcg_worker_id & ~UMCG_WQ_DISABLE_BIT,
+			UMCG_WE_EXIT);
+	/* Matches get_task_struct() in umcg_wake_worker(). */
+	put_task_struct(server);
+
+	curr->umcg_worker_id = 0;
+
+	return 0;
+}
+
+static int umcg_idle_loop(u64 abs_timeout);
+
+static int umcg_process_timedout_server(void)
+{
+	struct task_struct *curr = current;
+
+	spin_lock(&curr->mm->umcg_lock);
+	if (!list_empty(&curr->umcg_idle_server_list))
+		list_del_init(&curr->umcg_idle_server_list);
+	spin_unlock(&curr->mm->umcg_lock);
+
+	atomic_long_set(&curr->umcg_status, UMCG_SERVER_RUNNING);
+	return -ETIMEDOUT;
+}
+
+/*
+ * Wait until the worker is scheduled by the server or killed.
+ * Note that non-fatal signals do not break this loop.
+ */
+static void umcg_worker_wait(void)
+{
+	struct task_struct *curr = current;
+	u64 curr_status;
+
+	umcg_wq_work_disable(curr);  /* Avoid recursion. */
+	while (true) {
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		/* Must check status after setting task state to INTERRUPTIBLE */
+		curr_status = atomic_long_read_acquire(&curr->umcg_status);
+		if ((curr_status & UMCG_STATUS_RUNNING_MASK) ||
+				fatal_signal_pending(curr))
+			break;
+
+		freezable_schedule();
+	}
+
+	__set_current_state(TASK_RUNNING);
+	umcg_wq_work_enable(curr);
+	WRITE_ONCE(curr->umcg_worker_jiffies, jiffies);
+}
+
+static int umcg_process_timedout_worker(u64 old_status)
+{
+	u64 prev_status = old_status;
+	struct task_struct *curr = current;
+
+	if (!(prev_status & (UMCG_WORKER_RUNNABLE | UMCG_WORKER_SWAPPING)))
+		goto kill;
+
+	prev_status = atomic_long_cmpxchg(&curr->umcg_status, old_status,
+			old_status | UMCG_WORKER_TIMEOUT);
+	if (prev_status == old_status)
+		goto wait;
+
+	/*
+	 * cmpxchg above could have failed because of two racing events:
+	 * (a) a server attaching to the worker and markign it RUNNING, and
+	 * (b) a server making SWAPPING -> RUNNABLE change.
+	 */
+	if (prev_status == UMCG_WORKER_RUNNING)
+		goto run;
+
+	if (prev_status != UMCG_WORKER_RUNNABLE)
+		goto kill;
+
+	/* The second attempt. */
+	prev_status = atomic_long_cmpxchg(&curr->umcg_status, prev_status,
+			UMCG_WORKER_RUNNABLE | UMCG_WORKER_TIMEOUT);
+
+	/* Could have raced with a server attaching. */
+	if (prev_status == UMCG_WORKER_RUNNING)
+		goto run;
+	else if (prev_status != UMCG_WORKER_RUNNABLE) {
+		/*
+		 * RUNNABLE can only become RUNNING concurrently; RUNNING workers
+		 * change their status only "inline", while running, so _this_
+		 * worker is guaranteed to be RUNNABLE at this point.
+		 */
+		UMCG_WARN("unexpected status 0x%llx", prev_status);
+		goto kill;
+	}
+
+wait:
+	if (prev_status != UMCG_WORKER_SWAPPING) {
+		/* SWAPPING workers are already on the list. */
+		umcg_enqueue_idle_worker(curr);
+		/* Serialize the enqueue above and the wake below. */
+		umcg_wake_idle_server();
+	}
+
+	umcg_worker_wait();  /* Wait for a server to schedule the worker. */
+	return -ETIMEDOUT;
+
+run:
+	WRITE_ONCE(curr->umcg_worker_jiffies, jiffies);
+	return 0;
+
+kill:
+	UMCG_WARN("bad status: 0x%llx", prev_status);
+	force_sig(SIGKILL);
+	return -EINVAL;
+}
+
+static int umcg_idle_loop(u64 abs_timeout)
+{
+	int ret;
+	struct hrtimer_sleeper timeout;
+	struct task_struct *curr = current;
+	const bool worker = umcg_worker(curr);
+	u64 umcg_status;
+
+	/* Elide workqueue handlers. */
+	if (worker)
+		umcg_wq_work_disable(curr);
+
+	if (abs_timeout) {
+		/*
+		 * CLOCK_REALTIME is used here for historical reasons (SwitchTo).
+		 * In the future we will pass an additional parameter
+		 * that specifies which clock to use, as suggested by peterz@.
+		 */
+		hrtimer_init_sleeper_on_stack(&timeout, CLOCK_REALTIME,
+				HRTIMER_MODE_ABS);
+
+		hrtimer_set_expires_range_ns(&timeout.timer, (s64)abs_timeout,
+				curr->timer_slack_ns);
+	}
+
+	while (true) {
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		/* Must check status after setting task state to INTERRUPTIBLE */
+		umcg_status = atomic_long_read_acquire(&curr->umcg_status);
+
+		ret = 0;
+		if (umcg_status & UMCG_STATUS_RUNNING_MASK)
+			goto out;
+
+		ret = -EINTR;
+		if (signal_pending(curr)) {
+			set_tsk_thread_flag(curr, TIF_NOTIFY_RESUME);
+			goto out;
+		}
+
+		if (abs_timeout)
+			hrtimer_sleeper_start_expires(&timeout, HRTIMER_MODE_ABS);
+
+		if (!abs_timeout || timeout.task)
+			freezable_schedule();
+
+		umcg_status = atomic_long_read_acquire(&curr->umcg_status);
+
+		ret = 0;
+		if (umcg_status & UMCG_STATUS_RUNNING_MASK)
+			goto out;
+
+		ret = -ETIMEDOUT;
+		if (abs_timeout && !timeout.task)
+			goto out;
+	}
+
+out:
+	__set_current_state(TASK_RUNNING);
+
+	if (abs_timeout) {
+		hrtimer_cancel(&timeout.timer);
+		destroy_hrtimer_on_stack(&timeout.timer);
+	}
+
+	if (worker) {
+		umcg_wq_work_enable(curr);
+
+		if (ret == -ETIMEDOUT)
+			return umcg_process_timedout_worker(umcg_status);
+
+		WRITE_ONCE(curr->umcg_worker_jiffies, jiffies);
+	} else {
+		if (ret == -ETIMEDOUT)
+			return umcg_process_timedout_server();
+	}
+	return ret;
+}
+
+static int umcg_server_idle_loop(u64 abs_timeout, u64 __user *events, int event_sz)
+{
+	struct task_struct *curr = current;
+	u64 __user *next_event = events;
+	int ret;
+	u64 curr_event;
+
+	/* "1" is used for non-blocking polls. */
+	if (abs_timeout == 1)
+		ret = umcg_process_timedout_server();
+	else
+		ret = umcg_idle_loop(abs_timeout);
+
+	if (ret && (ret != -ETIMEDOUT))
+		return ret;
+
+	if (!event_sz)
+		return ret;
+
+	curr_event = atomic_long_xchg(&curr->umcg_worker_event, 0);
+	if (curr_event) {
+		if (put_user(curr_event, next_event))
+			return -EFAULT;
+		++next_event;
+		--event_sz;
+	}
+
+	/*
+	 * Sometimes we need to deliver two events, so we iterate until
+	 * event_sz > 1 and not until event_sz > 0.
+	 */
+	while (event_sz > 1) {
+		struct task_struct *worker = umcg_dequeue_idle_worker();
+		u64 worker_status, worker_id, event;
+		u64 extra_event = 0;
+
+		if (!worker)
+			break;
+
+		worker_status = atomic_long_xchg(&worker->umcg_status,
+				UMCG_WORKER_RUNNABLE);
+
+		if (worker_status == UMCG_WORKER_WAKING) {
+			struct task_struct *maybe_waiting_server;
+
+			event = (u64)UMCG_WE_WAKE;
+			/* RCU lock to prevent maybe_waiting_server exiting. */
+			rcu_read_lock();
+			maybe_waiting_server = READ_ONCE(worker->umcg_server);
+
+			if (maybe_waiting_server) {
+				u64 server_status = atomic_long_cmpxchg(
+						&maybe_waiting_server->umcg_status,
+						UMCG_SERVER_WAIT,
+						UMCG_SERVER_RUNNING);
+
+				if (server_status == UMCG_SERVER_WAIT) {
+					try_to_wake_up(maybe_waiting_server,
+							TASK_NORMAL, 0);
+				}
+			}
+			rcu_read_unlock();
+		} else if (worker_status == UMCG_WORKER_SWAPPING)
+			event = (u64)UMCG_WE_WAIT;
+		else if (worker_status ==
+				(UMCG_WORKER_RUNNABLE | UMCG_WORKER_TIMEOUT))
+			event = (u64)UMCG_WE_TIMEOUT;
+		else if (worker_status ==
+				(UMCG_WORKER_SWAPPING | UMCG_WORKER_TIMEOUT)) {
+			event = (u64)UMCG_WE_WAIT;
+			extra_event = (u64)UMCG_WE_TIMEOUT;
+		} else {
+			UMCG_WARN("%s:%d: Unexpected worker status: w %d s %llx",
+				__FILE__, __LINE__,
+				worker->pid, worker_status);
+			force_sig(SIGKILL);
+			return -EINVAL;
+		}
+
+		worker_id = READ_ONCE(worker->umcg_worker_id) & ~UMCG_WQ_DISABLE_BIT;
+		if (put_user(worker_id | event, next_event)) {
+			force_sig(SIGKILL);
+			return -EFAULT;
+		}
+
+		++next_event;
+		--event_sz;
+
+		if (extra_event != 0) {
+			if (put_user(worker_id | extra_event, next_event)) {
+				force_sig(SIGKILL);
+				return -EFAULT;
+			}
+
+			++next_event;
+			--event_sz;
+		}
+	}
+
+	if (next_event != events)
+		ret = 0;  /* Have results. */
+
+	if (event_sz > 0) {
+		if (put_user((u64)0, next_event)) {
+			force_sig(SIGKILL);
+			return -EFAULT;
+		}
+	}
+
+	return ret;
+}
+
+/**
+ * umcg_wakeup_allowed - check whether @current can wake @tsk.
+ *
+ * Currently a placeholder that allows wakeups within a single process
+ * only (same mm). In the future the requirement might be relaxed (securely).
+ */
+static bool umcg_wakeup_allowed(struct task_struct *tsk)
+{
+	WARN_ON_ONCE(!rcu_read_lock_held());
+
+	if (tsk->mm && tsk->mm == current->mm &&
+			(atomic_long_read(&tsk->umcg_status) != 0))
+		return true;
+
+	return false;
+}
+
+static long umcg_server_wait_for_worker(pid_t worker_tid)
+{
+	struct task_struct *worker;
+	struct task_struct *server = current;
+	u64 server_status, worker_status;
+	int ret = 0;
+
+	worker = find_get_task_by_vpid(worker_tid);
+	if (!worker)
+		return -ESRCH;
+
+	if (!umcg_wakeup_allowed(worker)) {
+		put_task_struct(worker);
+		return -EPERM;
+	}
+
+	server_status = atomic_long_xchg(&server->umcg_status, UMCG_SERVER_WAIT);
+	if (server_status != UMCG_SERVER_RUNNING) {
+		put_task_struct(worker);
+		UMCG_WARN("bad status: %llu", server_status);
+		force_sig(SIGKILL);
+		return -EINVAL;
+	}
+
+	worker_status = atomic_long_read_acquire(&worker->umcg_status);
+	if (worker_status != UMCG_WORKER_BLOCKED) {
+		put_task_struct(worker);
+		atomic_long_set_release(&server->umcg_status, UMCG_SERVER_RUNNING);
+		return 0;
+	}
+
+	if (xchg(&worker->umcg_server, get_task_struct(server)) != NULL) {
+		UMCG_WARN("bad worker state");
+		force_sig(SIGKILL);
+		return -EINVAL;
+	}
+
+	/* Pairs with atomic_long_xchg() in umcg_wq_worker_sleeping() */
+	smp_mb();
+
+	/* Check again. */
+	worker_status = atomic_long_read_acquire(&worker->umcg_status);
+	if (worker_status != UMCG_WORKER_BLOCKED) {
+		struct task_struct *server_now = xchg(&worker->umcg_server, NULL);
+
+		put_task_struct(worker);
+		if (server_now == server)
+			put_task_struct(server);
+		else if (server_now && (server_now != server)) {
+			/*
+			 * server_now can be NULL if the worker woke concurrently.
+			 * But if it is not equal to server/current, the userspace
+			 * screwed up.
+			 */
+			force_sig(SIGKILL);
+			return -EINVAL;
+		}
+		atomic_long_set_release(&server->umcg_status, UMCG_SERVER_RUNNING);
+		return 0;
+	}
+
+	while (true) {
+		if (signal_pending(server)) {
+			atomic_long_set_release(&server->umcg_status,
+					UMCG_SERVER_RUNNING);
+			ret = -EINTR;
+			goto out;
+		}
+
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		/* Must check status after setting task state to INTERRUPTIBLE */
+		server_status = atomic_long_read_acquire(&server->umcg_status);
+
+		if (server_status == UMCG_SERVER_RUNNING)
+			goto out;
+
+		freezable_schedule();
+	}
+
+out:
+	__set_current_state(TASK_RUNNING);
+	if (xchg(&worker->umcg_server, NULL) == server)
+		put_task_struct(server);
+	put_task_struct(worker);
+	return ret;
+}
+
+static long umcg_wake(u64 flags, pid_t next_tid, u64 abs_timeout,
+			u64 __user *events, int event_sz)
+{
+	if (flags || next_tid || abs_timeout || (u64) events || event_sz)
+		return -EINVAL;
+
+	if (umcg_wake_idle_server())
+		return 0;
+
+	return -EAGAIN;
+}
+
+static long umcg_wait(u64 flags, pid_t next_tid, u64 abs_timeout,
+			u64 __user *events, int event_sz)
+{
+	struct task_struct *curr = current;
+	u64 curr_status;
+
+	curr_status = atomic_long_read_acquire(&curr->umcg_status);
+
+	if (curr_status == UMCG_SERVER_RUNNING) {
+		if (next_tid) {
+			if (event_sz || abs_timeout || events)
+				return -EINVAL;
+			if (flags && (flags != UMCG_WAIT_FLAG_INTERRUPTED))
+				return -EINVAL;
+			return umcg_server_wait_for_worker(next_tid);
+		}
+
+		/*
+		 * Idle server was woken, but then interrupted before it could
+		 * collect/process worker events. The server could have been
+		 * in the "run worker" mode, and thus having event_sz == 1.
+		 */
+		if (flags == UMCG_WAIT_FLAG_INTERRUPTED)
+			return umcg_server_idle_loop(abs_timeout, events, event_sz);
+
+		/* Threre is no use case for less than two event slots. */
+		if (event_sz < 2)
+			return -EINVAL;
+
+		if (flags != 0)
+			return -EINVAL;
+
+		if (atomic_long_xchg(&curr->umcg_worker_event, 0)) {
+			UMCG_WARN("bad event");
+			goto kill;
+		}
+
+		curr_status = atomic_long_xchg(&curr->umcg_status,
+				UMCG_SERVER_IDLE);
+
+		if (curr_status != UMCG_SERVER_RUNNING) {
+			UMCG_WARN("bad status: %llu", curr_status);
+			goto kill;
+		}
+
+		umcg_enqueue_idle_server();
+		return umcg_server_idle_loop(abs_timeout, events, event_sz);
+	} else if (curr_status == UMCG_SERVER_IDLE) {
+		if (flags != UMCG_WAIT_FLAG_INTERRUPTED)
+			return -EINVAL;
+		/* The wait was interrupted by a signal. */
+		return umcg_server_idle_loop(abs_timeout, events, event_sz);
+	} else if (curr_status == UMCG_SERVER_ATTACHED) {
+		if (flags != UMCG_WAIT_FLAG_INTERRUPTED)
+			return -EINVAL;
+		/* The wait was interrupted by a signal. */
+		return umcg_server_idle_loop(abs_timeout, events, event_sz);
+	} else if (curr_status == UMCG_WORKER_RUNNING) {
+		struct task_struct *server = READ_ONCE(curr->umcg_server);
+
+		/* The wait was interrupted by a signal. */
+		if (flags == UMCG_WAIT_FLAG_INTERRUPTED)
+			return 0;
+
+		if (flags != 0)
+			return -EINVAL;
+
+		if (events || event_sz || next_tid)
+			return -EINVAL;
+
+		if (server != xchg(&curr->umcg_server, NULL)) {
+			/* This can't happen, but we are extra catious. */
+			UMCG_WARN("bad worker state");
+			goto kill;
+		}
+		curr_status = atomic_long_xchg(&curr->umcg_status,
+				UMCG_WORKER_RUNNABLE);
+		if (curr_status != UMCG_WORKER_RUNNING) {
+			UMCG_WARN("bad status: %llx", curr_status);
+			put_task_struct(server);
+			goto kill;
+		}
+
+		umcg_wake_server(server, curr->umcg_worker_id & ~UMCG_WQ_DISABLE_BIT,
+				UMCG_WE_WAIT);
+		/* Matches get_task_struct() in umcg_wake_worker(). */
+		put_task_struct(server);
+		return umcg_idle_loop(abs_timeout);
+	} else if (curr_status == UMCG_WORKER_RUNNABLE) {
+		if (flags != UMCG_WAIT_FLAG_INTERRUPTED)
+			return -EINVAL;
+		/* The wait was interrupted by a signal. */
+		return umcg_idle_loop(abs_timeout);
+	} else if (curr_status == UMCG_WORKER_SWAPPING) {
+		if (flags != UMCG_WAIT_FLAG_INTERRUPTED)
+			return -EINVAL;
+		/* The wait was interrupted by a signal. */
+		return umcg_idle_loop(abs_timeout);
+	} else {
+		UMCG_WARN("bad status: 0x%llx", curr_status);
+		goto kill;
+	}
+
+kill:
+	force_sig(SIGKILL);
+	return -EINVAL;
+}
+
+static void umcg_wake_worker(struct task_struct *worker,
+		struct task_struct *server)
+{
+	u64 umcg_status;
+
+	/*
+	 * Assign the server before the status change: a RUNNING worker
+	 * must have a server assigned.
+	 *
+	 * get_task_struct() below matches put_task_struct() in
+	 * wq_worker_sleeping(), umcg_wait(), and umcg_ctx_switch().
+	 */
+	WRITE_ONCE(worker->umcg_server, get_task_struct(server));
+
+	/* Set jiffies to zero to avoid preemption. */
+	WRITE_ONCE(worker->umcg_worker_jiffies, 0);
+	umcg_status = atomic_long_xchg(&worker->umcg_status,
+			UMCG_WORKER_RUNNING);
+	if (umcg_status != UMCG_WORKER_RUNNABLE) {
+		UMCG_WARN("w %d bad status: %llx", worker->pid, umcg_status);
+		force_sig(SIGKILL);
+		return;
+	}
+
+	try_to_wake_up(worker, TASK_NORMAL, WF_CURRENT_CPU);
+}
+
+/*
+ * Allows server->worker or worker->worker context switching.
+ *
+ * In the future, we may allow server->server context switching.
+ */
+static long umcg_ctx_switch(u64 flags, pid_t next_tid, u64 abs_timeout,
+				u64 __user *events, int event_sz)
+{
+	struct task_struct *curr = current;
+	struct task_struct *next;
+	u64 curr_status;
+
+	if (flags)
+		return -EINVAL;
+
+	rcu_read_lock();
+	next = find_task_by_vpid(next_tid);
+	if (!next) {
+		rcu_read_unlock();
+		return -ESRCH;
+	}
+
+	if (!umcg_wakeup_allowed(next)) {
+		rcu_read_unlock();
+		return -EPERM;
+	}
+
+	curr_status = atomic_long_read_acquire(&curr->umcg_status);
+
+	if (curr_status == UMCG_WORKER_RUNNING) {
+		struct task_struct *server = READ_ONCE(curr->umcg_server);
+
+		if (events || event_sz) {
+			rcu_read_unlock();
+			return -EINVAL;
+		}
+
+		WRITE_ONCE(curr->umcg_server, NULL);
+		curr_status = atomic_long_xchg(&curr->umcg_status,
+				UMCG_WORKER_SWAPPING);
+		if (curr_status != UMCG_WORKER_RUNNING) {
+			put_task_struct(server);
+			UMCG_WARN("bad status: %llu", curr_status);
+			goto kill;
+		}
+
+		umcg_enqueue_idle_worker(curr);
+		umcg_wake_worker(next, server);
+
+		/* Matches get_task_struct() in umcg_wake_worker(). */
+		put_task_struct(server);
+		rcu_read_unlock();
+		umcg_wake_idle_server();  /* For the enqueued/curr worker. */
+		return umcg_idle_loop(abs_timeout);
+	}
+
+	if (curr_status == UMCG_SERVER_RUNNING) {
+		if (abs_timeout) {
+			rcu_read_unlock();
+			return -EINVAL;
+		}
+
+		curr_status = atomic_long_xchg(&curr->umcg_status,
+				UMCG_SERVER_ATTACHED);
+		if (curr_status != UMCG_SERVER_RUNNING) {
+			UMCG_WARN("bad status: %llu", curr_status);
+			goto kill;
+		}
+
+		umcg_wake_worker(next, curr);
+		rcu_read_unlock();
+		return umcg_server_idle_loop(abs_timeout, events, event_sz);
+	}
+
+	UMCG_WARN("bad status: %llu", curr_status);
+
+kill:
+	rcu_read_unlock();
+	force_sig(SIGKILL);
+	return -EINVAL;
+}
+
+SYSCALL_DEFINE6(gsys_umcg, u64, flags, u64, cmd, pid_t, next_tid,
+		u64, abs_timeout, u64 __user *, events, int, event_sz)
+{
+	switch (cmd) {
+	case UMCG_REGISTER_WORKER:
+		return umcg_register_worker(flags, next_tid, abs_timeout,
+						events, event_sz);
+	case UMCG_REGISTER_SERVER:
+		return umcg_register_server(flags, next_tid, abs_timeout,
+						events, event_sz);
+	case UMCG_UNREGISTER:
+		return umcg_unregister(flags, next_tid, abs_timeout,
+					events, event_sz);
+	case UMCG_WAKE:
+		return umcg_wake(flags, next_tid, abs_timeout,
+					events, event_sz);
+	case UMCG_WAIT:
+		return umcg_wait(flags, next_tid, abs_timeout,
+					events, event_sz);
+	case UMCG_CTX_SWITCH:
+		return umcg_ctx_switch(flags, next_tid, abs_timeout,
+					events, event_sz);
+	default:
+		return -EINVAL;
+	}
+}
+
+static void umcg_preempt(struct task_struct *curr)
+{
+	u64 prev_status;
+	struct task_struct *server = NULL;
+
+	server = xchg(&curr->umcg_server, NULL);
+	prev_status = atomic_long_cmpxchg(&curr->umcg_status,
+			UMCG_WORKER_RUNNING, UMCG_WORKER_RUNNABLE);
+	if (prev_status != UMCG_WORKER_RUNNING) {
+		UMCG_WARN("%s:%d: bad status: 0x%llx tid %u\n", __func__,
+				__LINE__, prev_status, curr->pid);
+		goto kill;
+	}
+
+	umcg_wake_server(server, curr->umcg_worker_id & ~UMCG_WQ_DISABLE_BIT,
+			UMCG_WE_PREEMPT);
+	/* Matches get_task_struct() in umcg_wake_worker(). */
+	put_task_struct(server);
+
+	umcg_worker_wait();  /* Wait for a server to schedule the worker. */
+	return;
+
+kill:
+	if (server)
+		put_task_struct(server);
+	WRITE_ONCE(curr->umcg_server, NULL);
+	force_sig(SIGKILL);
+}
+
+void umcg_notify_resume(void)
+{
+	struct task_struct *curr = current;
+	struct task_struct *maybe_waiting_server;
+	u64 curr_status;
+	u64 curr_jiffies;
+
+	if (!umcg_worker(curr))
+		return;
+
+	if (fatal_signal_pending(curr))
+		return;
+
+	curr_status = atomic_long_read_acquire(&curr->umcg_status);
+	curr_jiffies = READ_ONCE(curr->umcg_worker_jiffies);
+
+	if ((curr_status == UMCG_WORKER_RUNNING) && (curr_jiffies != 0) &&
+			((jiffies - curr_jiffies) >=
+				CONFIG_UMCG_PREEMPT_JIFFIES)) {
+		umcg_preempt(curr);
+		return;
+	}
+
+	/*
+	 * Runnable workers can be interrupted. Note that this only applies
+	 * to RUNNABLE workers in umcg_wait(); workers that become RUNNABLE
+	 * once they are taken off the idle worker queue are in
+	 * umcg_worker_wait() and will not return to the userspace until
+	 * they are scheduled by a server to run.
+	 */
+	if (curr_status != UMCG_WORKER_BLOCKED)
+		return;
+
+	/* RCU lock to prevent maybe_waiting_server exiting. */
+	rcu_read_lock();
+	/*
+	 * Read the server before changing the worker status, as after
+	 * the worker's status changes out of BLOCKED, a lot of concurrent
+	 * activity might happen.
+	 */
+	maybe_waiting_server = READ_ONCE(curr->umcg_server);
+
+	curr_status = atomic_long_xchg(&curr->umcg_status, UMCG_WORKER_WAKING);
+	if (curr_status != UMCG_WORKER_BLOCKED) {
+		rcu_read_unlock();
+		UMCG_WARN("bad status: %llu", curr_status);
+		force_sig(SIGKILL);
+		return;
+	}
+
+	/*
+	 * Enqueue the worker even if there is a waiting server, as the wakeup
+	 * event still needs to be delivered.
+	 */
+	umcg_enqueue_idle_worker(curr);
+	if (maybe_waiting_server) {
+		u64 server_status = atomic_long_cmpxchg(
+				&maybe_waiting_server->umcg_status,
+				UMCG_SERVER_WAIT,
+				UMCG_SERVER_RUNNING);
+
+		if (server_status == UMCG_SERVER_WAIT) {
+			try_to_wake_up(maybe_waiting_server, TASK_NORMAL,
+					WF_CURRENT_CPU);
+		}
+	} else {
+		umcg_wake_idle_server();
+	}
+	rcu_read_unlock();
+
+	umcg_worker_wait();  /* Wait for a server to schedule the worker. */
+}
+
+void umcg_wq_worker_sleeping(struct task_struct *curr)
+{
+	u64 prev_status;
+	struct task_struct *server = NULL;
+
+	prev_status = atomic_long_cmpxchg(&curr->umcg_status,
+			UMCG_WORKER_RUNNING, UMCG_WORKER_BLOCKED);
+	server = READ_ONCE(curr->umcg_server);
+	if (prev_status != UMCG_WORKER_RUNNING) {
+		UMCG_WARN("%s:%d: bad status: 0x%llx tid %u\n", __func__,
+				__LINE__, prev_status, curr->pid);
+		goto kill;
+	}
+	WRITE_ONCE(curr->umcg_server, NULL);
+
+	umcg_wake_server(server, curr->umcg_worker_id & ~UMCG_WQ_DISABLE_BIT,
+			UMCG_WE_BLOCK);
+	/* Matches get_task_struct() in umcg_wake_worker(). */
+	put_task_struct(server);
+
+	/* Trigger UMCG_WE_WAKE. */
+	set_tsk_thread_flag(curr, TIF_NOTIFY_RESUME);
+	return;
+
+kill:
+	if (server)
+		put_task_struct(server);
+	WRITE_ONCE(curr->umcg_server, NULL);
+	force_sig(SIGKILL);
+}
+
+void umcg_handle_exit(void)
+{
+	struct task_struct *curr = current;
+	u64 curr_status = atomic_long_read_acquire(&curr->umcg_status);
+
+	if (!curr_status)
+		return;
+
+	if (!umcg_unregister(0, 0, 0, NULL, 0))
+		return;
+
+	/* Normal unregister failed, do a hard cleanup. */
+	if (umcg_worker(curr)) {
+		struct task_struct *server = READ_ONCE(curr->umcg_server);
+
+		if (server && (curr_status == UMCG_WORKER_RUNNING))
+			put_task_struct(server);
+		else if (server)
+			UMCG_WARN("UMCG worker exiting in state 0x%llx",
+				curr_status);
+	}
+	atomic_long_set_release(&curr->umcg_status, 0);
+}
+
+void umcg_tick(struct task_struct *curr)
+{
+	if (umcg_wq_work(curr)) {
+		u64 curr_jiffies = READ_ONCE(curr->umcg_worker_jiffies);
+
+		if ((curr_jiffies != 0) && ((jiffies - curr_jiffies) >=
+					CONFIG_UMCG_PREEMPT_JIFFIES))
+			set_tsk_thread_flag(curr, TIF_NOTIFY_RESUME);
+	}
+}
